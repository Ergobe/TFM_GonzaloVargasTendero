{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe6a29b-9eb9-4744-aa32-544f60294565",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae3baa-03f5-4257-bd0d-b0aad83e2ea2",
   "metadata": {},
   "source": [
    "## 3. MOP\n",
    "\n",
    "### 3.1. Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff92533-1e76-4f8f-9cf7-d0d175ed192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías necesarias\n",
    "import os\n",
    "import re  # Import the regular expression module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1922bfe-c125-49ed-9d94-97ac70c11edf",
   "metadata": {},
   "source": [
    "### 3.2. Cargar y convertir los datos preprocesados en matrices X, M y P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b369df5f-7f54-4a94-8e28-e7a51fc84316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\DB_MOP\\design_DB_preprocessed_200_Optimizado.csv\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Figuras_MOP\\200_MOT_Optimizado\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\200_MOT_Optimizado\n"
     ]
    }
   ],
   "source": [
    "# Definir las rutas base y de las carpetas\n",
    "base_path = os.getcwd()  # Se asume que el notebook se ejecuta desde la carpeta 'MOP'\n",
    "db_path = os.path.join(base_path, \"DB_MOP\")\n",
    "fig_path = os.path.join(base_path, \"Figuras_MOP\")\n",
    "model_path = os.path.join(base_path, \"Modelos_MOP\")\n",
    "\n",
    "# Ruta al archivo de la base de datos\n",
    "data_file = os.path.join(db_path, \"design_DB_preprocessed_200_Optimizado.csv\")\n",
    "print(data_file)\n",
    "\n",
    "# Ruta al archivo de las figuras\n",
    "figure_path = os.path.join(fig_path, \"200_MOT_Optimizado\")\n",
    "print(figure_path)\n",
    "\n",
    "# Ruta al archivo de los modelos\n",
    "modelo_path = os.path.join(model_path, \"200_MOT_Optimizado\")\n",
    "print(modelo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5990deb-90f7-4e71-9caa-7a16036bf407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Lectura del archivo CSV\n",
    "try:\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(\"Archivo cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado. Revisa la ruta del archivo.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Problema al analizar el archivo CSV. Revisa el formato del archivo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado: {e}\")\n",
    "\n",
    "# Función para limpiar nombres de archivo inválidos\n",
    "def clean_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa110f43-c898-44ba-8701-3acadbccc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa las columnas en matrices X, M y P\n",
    "X_cols = [col for col in df.columns if col.startswith('x')]\n",
    "M_cols = [col for col in df.columns if col.startswith('m')]\n",
    "P_cols = [col for col in df.columns if col.startswith('p')]\n",
    "\n",
    "X = df[X_cols].copy()\n",
    "M = df[M_cols].copy()\n",
    "P = df[P_cols].copy()\n",
    "\n",
    "# Transforma todos los datos de X, M y P a numéricos\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "for col in M.columns:\n",
    "    M[col] = pd.to_numeric(M[col], errors='coerce')\n",
    "\n",
    "for col in P.columns:\n",
    "    P[col] = pd.to_numeric(P[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632978ca-2b07-400a-9d96-806bcb147571",
   "metadata": {},
   "source": [
    "### 3.3. Entrenamiento para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1012dc02-4ab8-4f06-a9a8-2b5eac0a187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de entrada: ['x1::OSD', 'x2::Dint', 'x3::L', 'x4::tm', 'x5::hs2', 'x6::wt', 'x7::Nt', 'x8::Nh', 'm1::Drot', 'm2::Dsh', 'm3::he', 'm4::Rmag', 'm5::Rs', 'm6::GFF']\n",
      "Variables de salida: ['p1::W', 'p4::GFF', 'p5::BSP_T', 'p6::BSP_n', 'p7::BSP_Mu', 'p8::MSP_n', 'p9::UWP_Mu']\n"
     ]
    }
   ],
   "source": [
    "# Las variables de salida será las etiquetas de la matriz P\n",
    "outputs = [col for col in P.columns]\n",
    "outputs.remove('p2::Tnom')\n",
    "outputs.remove('p3::nnom')\n",
    "\n",
    "# Concatena las matrices X y M\n",
    "X_M = pd.concat([X, M], axis=1)\n",
    "\n",
    "# Las entradas serán el resto de las columnas (tanto X como M)\n",
    "features = [col for col in X_M.columns]\n",
    "\n",
    "print(\"Variables de entrada:\", features)\n",
    "print(\"Variables de salida:\", outputs)\n",
    "\n",
    "X = df[features]\n",
    "Y = df[outputs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4802ba67-37c7-44ee-9638-4ab6a2d8bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de datos\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_M)\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a562a51b-4edf-4b9d-b9f2-f88c53aad297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir la base de datos en entrenamiento y prueba (80% / 20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c34123b0-f51b-4f64-9c81-cc44f9439599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Escalar datos de entrada para modelos sensibles a la escala (por ejemplo, GPR)\n",
    "# =============================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "X_scaled = pd.DataFrame(scaler.transform(X), columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b392c4e-7a09-4968-a500-fa93866a75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Funciones de cálculo\n",
    "# =============================================================================\n",
    "def compute_CoP(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el Coefficient of Prognosis (CoP) según la fórmula:\n",
    "       CoP = (Pearson_corr(y_true, y_pred))^2\n",
    "    \"\"\"\n",
    "    if np.std(y_true) == 0 or np.std(y_pred) == 0:\n",
    "        return np.nan\n",
    "    r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    return r ** 2\n",
    "\n",
    "def compute_standardized_coefficients(model, X, y):\n",
    "    \"\"\"\n",
    "    Para modelos lineales (que tienen atributo coef_),\n",
    "    calcula los coeficientes estandarizados:\n",
    "        coef_std = coef * (std(X) / std(y))\n",
    "    \"\"\"\n",
    "    coef = model.coef_.ravel()\n",
    "    std_X = X.std().values\n",
    "    std_y = y.std()\n",
    "    return coef * std_X / std_y\n",
    "\n",
    "def compute_pij(model, X, dataset=\"train\"):\n",
    "    \"\"\"\n",
    "    Calcula p_ij para un modelo entrenado, es decir, para cada variable de entrada\n",
    "    se calcula la correlación de Pearson entre la predicción del modelo (Ŝ)\n",
    "    y esa variable, según la fórmula:\n",
    "    \n",
    "         p_ij = (1/(N-1)) * Σ_k ((ŷ(k) - μ_ŷ) (x_j(k) - μ_xj)) / (σ_ŷ σ_xj)\n",
    "         \n",
    "    Se evalúa en el conjunto X (por ejemplo, X_train).\n",
    "    Si alguna variable es constante (std=0), se asigna NaN para evitar errores.\n",
    "    \"\"\"\n",
    "    # Obtener predicciones sobre X (se asume que la salida es unidimensional)\n",
    "    y_pred = model.predict(X).ravel()\n",
    "    pij = []\n",
    "    for col in X.columns:\n",
    "        std_feature = X[col].std()\n",
    "        if std_feature == 0:\n",
    "            pij.append(np.nan)\n",
    "        else:\n",
    "            corr = np.corrcoef(y_pred, X[col].values)[0, 1]\n",
    "            pij.append(corr)\n",
    "    return np.array(pij)\n",
    "\n",
    "def compute_permutation_importance(model, X, y):\n",
    "    \"\"\"\n",
    "    Para modelos no lineales (o en general), se utiliza la importancia por\n",
    "    permutación para estimar el efecto de cada variable.\n",
    "    \"\"\"\n",
    "    result = permutation_importance(model, X, y, scoring=\"r2\", n_repeats=10, random_state=0)\n",
    "    return result.importances_mean\n",
    "\n",
    "def plot_heatmap(matrix, col_labels, row_labels, title, ax=None):\n",
    "    \"\"\"\n",
    "    Representa un mapa de calor a partir de la matriz dada usando seaborn.\n",
    "    Si se proporciona 'ax', se dibuja en ese subplot.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    sns.heatmap(matrix, annot=True, fmt=\".2f\", xticklabels=col_labels, \n",
    "                yticklabels=row_labels, cmap=\"viridis\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6425a49-3a6c-4903-9247-3825a61ae233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número óptimo de componentes para modelar PLS es: 9\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Definir modelos subrogados\n",
    "# Se utilizan: PLS, regresión lineal (LR) y modelo de Kriging (GPR)\n",
    "# =============================================================================\n",
    "\n",
    "#==============================================================================\n",
    "# PLS\n",
    "n_features = X_train.shape[1]\n",
    "# Determinar número óptimo de componentes usando validación cruzada\n",
    "mse = []\n",
    "componentes = np.arange(1, min(len(X.columns), 20))\n",
    "\n",
    "for n in componentes:\n",
    "    pls = PLSRegression(n_components=n)\n",
    "    scores = cross_val_score(pls, X_train, Y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse.append(-scores.mean())\n",
    "# Selección óptima de componentes (menor MSE)\n",
    "n_componentes_optimos = componentes[np.argmin(mse)]\n",
    "print(f'El número óptimo de componentes para modelar PLS es: {n_componentes_optimos}')\n",
    "#==============================================================================\n",
    "\n",
    "#==============================================================================\n",
    "# Kriging\n",
    "kernel = C(1.0, (1e-4, 1e4)) * RBF(1.0, (1e-3, 1e3)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-4, 1e2))\n",
    "#==============================================================================\n",
    "# \"GPR\": lambda: GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, normalize_y=True),\n",
    "# \"GPR\": lambda: GaussianProcessRegressor(kernel=RBF() + WhiteKernel(), random_state=42),\n",
    "# \"SVR\": lambda: MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "models = {\n",
    "    \"PLS\": lambda: PLSRegression(n_components=n_componentes_optimos),\n",
    "    \"LR\": lambda: LinearRegression(),\n",
    "    \"GPR\": lambda: GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, normalize_y=True),\n",
    "    \"SVR\": lambda: MultiOutputRegressor(SVR(kernel='rbf')),\n",
    "    \"RF\": lambda: RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928cebf1-2145-4ecd-a080-48ecd800bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionarios para almacenar resultados\n",
    "cop_results = {output: {} for output in outputs}      # CoP para cada modelo y salida\n",
    "pij_results = {output: {} for output in outputs}       # Parámetros p_ij para cada modelo y salida\n",
    "predictions = {output: {} for output in outputs}        # Predicciones en test para cada modelo y salida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b92bb8d0-1abe-426d-99aa-55941eef5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Definir modelos subrogados\n",
    "# Se usan: PLS, regresión lineal (LR), GaussianProcessRegressor (GPR) y SVR.\n",
    "# Para GPR y SVR se utilizarán datos escalados.\n",
    "# =============================================================================\n",
    "# Entrenar cada modelo para cada variable de salida\n",
    "for output in outputs:\n",
    "    # Se omite si la salida es constante (ya filtrada previamente)\n",
    "    if Y_train[output].std() == 0:\n",
    "        print(f\"Skipping output {output} because it is constant.\")\n",
    "        continue\n",
    "    \n",
    "    y_train = Y_train[output]\n",
    "    y_test = Y_test[output]\n",
    "    \n",
    "    for model_name, model_constructor in models.items():\n",
    "        model = model_constructor()\n",
    "        try:\n",
    "            # Para GPR y SVR usar datos escalados; para PLS y LR usar datos originales.\n",
    "            # Para SVR, convertir y_train a 2D para MultiOutputRegressor.\n",
    "            if model_name in [\"GPR\", \"SVR\"]:\n",
    "                if model_name == \"SVR\":\n",
    "                    model.fit(X_train_scaled, y_train.values.reshape(-1, 1))\n",
    "                else:\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name} for {output}: {e}\")\n",
    "            cop_results[output][model_name] = np.nan\n",
    "            pij_results[output][model_name] = np.full(n_features, np.nan)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            if model_name in [\"GPR\", \"SVR\"]:\n",
    "                y_pred_test = model.predict(X_test_scaled).ravel()\n",
    "            else:\n",
    "                y_pred_test = model.predict(X_test).ravel()\n",
    "            cop = compute_CoP(y_test, y_pred_test)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing CoP for {model_name} on {output}: {e}\")\n",
    "            cop = np.nan\n",
    "        cop_results[output][model_name] = cop\n",
    "        predictions[output][model_name] = y_pred_test\n",
    "        \n",
    "        try:\n",
    "            if model_name in [\"GPR\", \"SVR\"]:\n",
    "                pij = compute_pij(model, X_train_scaled)\n",
    "            else:\n",
    "                pij = compute_pij(model, X_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing p_ij for {model_name} on {output}: {e}\")\n",
    "            pij = np.full(n_features, np.nan)\n",
    "        pij_results[output][model_name] = pij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f52c56-4819-4f59-af26-d4dfa4743c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejores modelos por variable de salida:\n",
      "  p1::W: GPR (CoP = 1.000)\n",
      "  p4::GFF: GPR (CoP = 1.000)\n",
      "  p5::BSP_T: GPR (CoP = 0.992)\n",
      "  p6::BSP_n: GPR (CoP = 0.975)\n",
      "  p7::BSP_Mu: GPR (CoP = 0.958)\n",
      "  p8::MSP_n: GPR (CoP = 0.923)\n",
      "  p9::UWP_Mu: GPR (CoP = 0.873)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Comparar CoP y elegir el mejor modelo para cada variable de salida\n",
    "# =============================================================================\n",
    "best_models = {}\n",
    "for output in outputs:\n",
    "    # Se selecciona el modelo con mayor CoP\n",
    "    cop_dict = cop_results[output]\n",
    "    if len(cop_dict) == 0:\n",
    "        continue\n",
    "    best_model = max(cop_dict, key=lambda m: cop_dict[m] if not np.isnan(cop_dict[m]) else -np.inf)\n",
    "    best_models[output] = best_model\n",
    "    \n",
    "print(\"\\nMejores modelos por variable de salida:\")\n",
    "for output in outputs:\n",
    "    best_model = best_models.get(output)\n",
    "    if best_model is None:\n",
    "        print(f\"  {output}: No se evaluó ningún modelo (posiblemente la variable es constante o hubo error)\")\n",
    "    else:\n",
    "        print(f\"  {output}: {best_model} (CoP = {cop_results[output][best_model]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bebf768f-c37d-45b2-a905-e6da98d701fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar los CoP de cada modelo para cada variable de salida en subplots\n",
    "# =============================================================================\n",
    "n_out = len(outputs)\n",
    "ncols = 3\n",
    "nrows = ceil(n_out / ncols)\n",
    "fig1, axes1 = plt.subplots(nrows, ncols, figsize=(12, 6*nrows))\n",
    "axes1 = axes1.flatten() if n_out > 1 else [axes1]\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    model_names = list(cop_results[output].keys())\n",
    "    cop_vals = [cop_results[output][m] for m in model_names]\n",
    "    ax = axes1[i]\n",
    "    ax.bar(model_names, cop_vals, color=\"steelblue\")\n",
    "    ax.set_title(f\"CoP para {output}\", fontsize=14)\n",
    "    ax.set_ylabel(\"CoP\", fontsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02833ddb-438d-4491-a81f-cb8b589cab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar los p_ij en mapas de calor: filas = features, columnas = salidas\n",
    "# Se genera un heatmap para cada modelo evaluado\n",
    "# =============================================================================\n",
    "for model_name in models.keys():\n",
    "    # Construir una matriz de p_ij: filas = features, columnas = outputs\n",
    "    pij_matrix = []\n",
    "    valid_outputs = []\n",
    "    for output in outputs:\n",
    "        if model_name in pij_results[output]:\n",
    "            pij_matrix.append(pij_results[output][model_name])\n",
    "            valid_outputs.append(output)\n",
    "    if len(pij_matrix) == 0:\n",
    "        continue\n",
    "    pij_matrix = np.array(pij_matrix).T  # dimensiones: (n_features, n_outputs)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_heatmap(pij_matrix, col_labels=valid_outputs, row_labels=features,\n",
    "                 title=f\"Mapa de calor de p_ij para {model_name}\", ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6b8635-3588-4bd7-b72b-464b0928de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar el mapa de calor de la matriz de correlación de Pearson (entradas vs salidas)\n",
    "# =============================================================================\n",
    "pearson_matrix = np.zeros((len(features), len(outputs)))\n",
    "for i, feat in enumerate(features):\n",
    "    for j, out in enumerate(outputs):\n",
    "        if df[feat].std() == 0 or df[out].std() == 0:\n",
    "            pearson_matrix[i, j] = np.nan\n",
    "        else:\n",
    "            pearson_matrix[i, j] = np.corrcoef(df[feat], df[out])[0, 1]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_heatmap(pearson_matrix, col_labels=outputs, row_labels=features,\n",
    "             title=\"Matriz de correlación de Pearson (entradas vs salidas)\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4278bbc4-637d-4e53-9b62-e2180c7a8d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Con el mejor modelo (según CoP) para cada salida, reentrenar con todo el dataset\n",
    "# y generar un nuevo dataset con las predicciones óptimas\n",
    "# =============================================================================\n",
    "optimal_predictions = {}\n",
    "for output in outputs:\n",
    "    if output not in best_models:\n",
    "        continue\n",
    "    model_name = best_models[output]\n",
    "    model = models[model_name]()\n",
    "    y_full = df[output]\n",
    "    try:\n",
    "        model.fit(X, y_full)\n",
    "        y_pred_full = model.predict(X).ravel()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al reentrenar el modelo {model_name} para {output}: {e}\")\n",
    "        y_pred_full = np.full(len(df), np.nan)\n",
    "    optimal_predictions[output] = y_pred_full\n",
    "\n",
    "# Crear un nuevo DataFrame con las predicciones óptimas\n",
    "df_optimal = X.copy()\n",
    "for output in outputs:\n",
    "    if output in optimal_predictions:\n",
    "        df_optimal[f\"{output}_pred\"] = optimal_predictions[output]\n",
    "    else:\n",
    "        df_optimal[f\"{output}_pred\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b617bcc-91ae-4b21-b1aa-c2bee95ae9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Subplots para comparar, para cada salida, los valores FEA (reales) vs. la predicción\n",
    "# del modelo óptimo, mostrando R², CoP y MSE.\n",
    "# =============================================================================\n",
    "fig2, axes2 = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 6*nrows))\n",
    "axes2 = axes2.flatten() if n_out > 1 else [axes2]\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    y_true = df[output].values\n",
    "    y_pred = df_optimal[f\"{output}_pred\"].values\n",
    "    # Crear una máscara para filtrar filas sin NaN\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"Para la salida {output} no hay datos válidos para evaluar métricas.\")\n",
    "        continue\n",
    "    y_true_valid = y_true[mask]\n",
    "    y_pred_valid = y_pred[mask]\n",
    "    \n",
    "    r2 = r2_score(y_true_valid, y_pred_valid)\n",
    "    mse = mean_squared_error(y_true_valid, y_pred_valid)\n",
    "    cop_full = compute_CoP(y_true_valid, y_pred_valid)\n",
    "    \n",
    "    ax = axes2[i]\n",
    "    ax.scatter(y_true_valid, y_pred_valid, alpha=0.6, edgecolor=\"k\")\n",
    "    ax.plot([y_true_valid.min(), y_true_valid.max()],\n",
    "            [y_true_valid.min(), y_true_valid.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel(\"Simulación FEA\", fontsize=12)\n",
    "    ax.set_ylabel(\"Predicción subrogada\", fontsize=12)\n",
    "    ax.set_title(f\"{output}\\nR²={r2:.3f}, CoP={cop_full:.3f}, MSE={mse:.3e}\", fontsize=14)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a651b52f-57eb-469e-8da9-a04fe06537f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nuevo dataset generado: 'trained_database_optimal.csv'\n"
     ]
    }
   ],
   "source": [
    "# Guardar el nuevo dataset\n",
    "df_optimal.to_csv(\"trained_database_optimal.csv\", index=False)\n",
    "print(\"\\nNuevo dataset generado: 'trained_database_optimal.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f16aaaa-c9d4-4317-8336-e28ced107acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observación sobre MSE:\n",
      "Si los valores de MSE son muy superiores a 1e5, esto indica que, en promedio,\n",
      "la diferencia al cuadrado entre las predicciones y los valores reales es muy alta.\n",
      "Esto puede deberse a que:\n",
      "  - La escala de las variables de entrada o salida es muy amplia y no han sido normalizadas.\n",
      "  - El modelo no logra capturar correctamente la relación entre las entradas y la salida.\n",
      "  - Existen outliers o alta dispersión en los datos.\n",
      "En estos casos, es recomendable revisar la escala de los datos, aplicar normalización\n",
      "o transformaciones, o incluso reevaluar la idoneidad del modelo para esa variable.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Comentario sobre valores MSE muy altos\n",
    "# =============================================================================\n",
    "print(\"\\nObservación sobre MSE:\")\n",
    "print(\"Si los valores de MSE son muy superiores a 1e5, esto indica que, en promedio,\")\n",
    "print(\"la diferencia al cuadrado entre las predicciones y los valores reales es muy alta.\")\n",
    "print(\"Esto puede deberse a que:\")\n",
    "print(\"  - La escala de las variables de entrada o salida es muy amplia y no han sido normalizadas.\")\n",
    "print(\"  - El modelo no logra capturar correctamente la relación entre las entradas y la salida.\")\n",
    "print(\"  - Existen outliers o alta dispersión en los datos.\")\n",
    "print(\"En estos casos, es recomendable revisar la escala de los datos, aplicar normalización\")\n",
    "print(\"o transformaciones, o incluso reevaluar la idoneidad del modelo para esa variable.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
