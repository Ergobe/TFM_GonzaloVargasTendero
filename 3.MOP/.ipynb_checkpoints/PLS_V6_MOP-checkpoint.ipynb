{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe6a29b-9eb9-4744-aa32-544f60294565",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae3baa-03f5-4257-bd0d-b0aad83e2ea2",
   "metadata": {},
   "source": [
    "## 3. MOP\n",
    "\n",
    "### 3.1. Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff92533-1e76-4f8f-9cf7-d0d175ed192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. MOP - Modelo de Optimización y Prognosis\n",
    "# =============================================================================\n",
    "\n",
    "# Librerías necesarias\n",
    "import os\n",
    "import re  # Import the regular expression module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Preprocesamiento y modelado\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1922bfe-c125-49ed-9d94-97ac70c11edf",
   "metadata": {},
   "source": [
    "### 3.2. Cargar y convertir los datos preprocesados en matrices X, M y P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b369df5f-7f54-4a94-8e28-e7a51fc84316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\DB_MOP\\design_DB_preprocessed_200_Optimizado.csv\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Figuras_MOP\\200_MOT_Optimizado\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\200_MOT_Optimizado\n"
     ]
    }
   ],
   "source": [
    "# Definir las rutas base y de las carpetas\n",
    "base_path = os.getcwd()  # Se asume que el notebook se ejecuta desde la carpeta 'MOP'\n",
    "db_path = os.path.join(base_path, \"DB_MOP\")\n",
    "fig_path = os.path.join(base_path, \"Figuras_MOP\")\n",
    "model_path = os.path.join(base_path, \"Modelos_MOP\")\n",
    "\n",
    "# Ruta al archivo de la base de datos\n",
    "data_file = os.path.join(db_path, \"design_DB_preprocessed_200_Optimizado.csv\")\n",
    "print(data_file)\n",
    "\n",
    "# Ruta al archivo de las figuras\n",
    "figure_path = os.path.join(fig_path, \"200_MOT_Optimizado\")\n",
    "print(figure_path)\n",
    "\n",
    "# Ruta al archivo de los modelos\n",
    "modelo_path = os.path.join(model_path, \"200_MOT_Optimizado\")\n",
    "print(modelo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5990deb-90f7-4e71-9caa-7a16036bf407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Lectura del archivo CSV\n",
    "try:\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(\"Archivo cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado. Revisa la ruta del archivo.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Problema al analizar el archivo CSV. Revisa el formato del archivo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado: {e}\")\n",
    "\n",
    "# Función para limpiar nombres de archivo inválidos\n",
    "def clean_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa110f43-c898-44ba-8701-3acadbccc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa las columnas en matrices X, M y P\n",
    "X_cols = [col for col in df.columns if col.startswith('x')]\n",
    "M_cols = [col for col in df.columns if col.startswith('m')]\n",
    "P_cols = [col for col in df.columns if col.startswith('p')]\n",
    "\n",
    "X = df[X_cols].copy()\n",
    "M = df[M_cols].copy()\n",
    "P = df[P_cols].copy()\n",
    "\n",
    "# Transforma todos los datos de X, M y P a numéricos\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "for col in M.columns:\n",
    "    M[col] = pd.to_numeric(M[col], errors='coerce')\n",
    "\n",
    "for col in P.columns:\n",
    "    P[col] = pd.to_numeric(P[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632978ca-2b07-400a-9d96-806bcb147571",
   "metadata": {},
   "source": [
    "### 3.3. Entrenamiento para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1012dc02-4ab8-4f06-a9a8-2b5eac0a187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de entrada: ['x1::OSD', 'x2::Dint', 'x3::L', 'x4::tm', 'x5::hs2', 'x6::wt', 'x7::Nt', 'x8::Nh', 'm1::Drot', 'm2::Dsh', 'm3::he', 'm4::Rmag', 'm5::Rs', 'm6::GFF']\n",
      "Variables de salida: ['p1::W', 'p4::GFF', 'p5::BSP_T', 'p6::BSP_n', 'p7::BSP_Mu', 'p8::MSP_n', 'p9::UWP_Mu']\n"
     ]
    }
   ],
   "source": [
    "# Las variables de salida se toman de la matriz P. Se eliminan 'p2::Tnom' y 'p3::nnom'\n",
    "outputs = [col for col in P.columns]\n",
    "if 'p2::Tnom' in outputs: outputs.remove('p2::Tnom')\n",
    "if 'p3::nnom' in outputs: outputs.remove('p3::nnom')\n",
    "\n",
    "# Concatena las matrices X y M\n",
    "X_M = pd.concat([X, M], axis=1)\n",
    "\n",
    "# Las entradas serán el resto de las columnas (tanto X como M)\n",
    "features = [col for col in X_M.columns]\n",
    "\n",
    "print(\"Variables de entrada:\", features)\n",
    "print(\"Variables de salida:\", outputs)\n",
    "\n",
    "X = df[features]\n",
    "Y = df[outputs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c863f8dd-22be-42ac-9241-fa530feee6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de datos\n",
    "# Se utiliza StandardScaler para normalizar los datos (importante para algunos modelos)\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)  # Datos de entrada escalados\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y)  # Datos de salida escalados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a562a51b-4edf-4b9d-b9f2-f88c53aad297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Separamos los conjuntos de datos en entrenamiento y test.\n",
    "# =============================================================================\n",
    "# Separar conjuntos de entrenamiento y prueba (80% / 20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Para asegurar que los DataFrames tengan índices consecutivos, se reinician\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test  = X_test.reset_index(drop=True)\n",
    "Y_train = Y_train.reset_index(drop=True)\n",
    "Y_test  = Y_test.reset_index(drop=True)\n",
    "\n",
    "# También, para los datos escalados (se convierten a DataFrame para conservar nombres de columnas)\n",
    "X_train_scaled = pd.DataFrame(scaler_X.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled  = pd.DataFrame(scaler_X.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "Y_train_scaled = pd.DataFrame(scaler_Y.transform(Y_train), columns=Y_train.columns)\n",
    "Y_test_scaled  = pd.DataFrame(scaler_Y.transform(Y_test), columns=Y_test.columns)\n",
    "\n",
    "# Crear DataFrames para el conjunto completo escalado (usado en reentrenamiento final)\n",
    "X_scaled_df = pd.DataFrame(scaler_X.transform(X), columns=X.columns, index=X.index)\n",
    "Y_scaled_df = pd.DataFrame(scaler_Y.transform(Y), columns=Y.columns, index=Y.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e778b23-a2a2-4249-a031-51499101e265",
   "metadata": {},
   "source": [
    "### 3.4. Funciones de Cálculo (Algoritmos y Componentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b392c4e-7a09-4968-a500-fa93866a75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Funciones de cálculo\n",
    "# =============================================================================\n",
    "def compute_CoP(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el Coefficient of Prognosis (CoP) usando la fórmula:\n",
    "      CoP = (Pearson_corr(y_true, y_pred))^2\n",
    "    Se convierte y_true y y_pred a arrays NumPy para evitar ambigüedades.\n",
    "    Si la desviación estándar de alguno es 0, retorna NaN.\n",
    "    \"\"\"\n",
    "    y_true_arr = np.array(y_true)\n",
    "    y_pred_arr = np.array(y_pred)\n",
    "    if np.std(y_true_arr, axis=0) == 0 or np.std(y_pred_arr, axis=0) == 0:\n",
    "        return np.nan\n",
    "    r = np.corrcoef(y_true_arr, y_pred_arr)[0, 1]\n",
    "    return r ** 2\n",
    "\n",
    "def compute_standardized_coefficients(model, X, y):\n",
    "    \"\"\"\n",
    "    Para modelos lineales (con atributo coef_), calcula los coeficientes\n",
    "    estandarizados según:\n",
    "      coef_std = coef * (std(X) / std(y))\n",
    "    \"\"\"\n",
    "    coef = model.coef_.ravel()\n",
    "    std_X = X.std().values\n",
    "    std_y = y.std()\n",
    "    return coef * std_X / std_y\n",
    "\n",
    "def compute_pij(model, X):\n",
    "    \"\"\"\n",
    "    Calcula el parámetro p_ij para un modelo entrenado.\n",
    "    Para cada variable de entrada (columna en X), calcula la correlación de Pearson entre\n",
    "    las predicciones del modelo (ŷ) y esa variable.\n",
    "    \n",
    "    Se utiliza la fórmula:\n",
    "      p_ij = (1/(N-1)) * Σ((ŷ(k) - μ_ŷ)(x_j(k) - μ_xj)) / (σ_ŷ σ_xj)\n",
    "    \n",
    "    Si la variable es constante (σ=0), se asigna NaN.\n",
    "    Si X no es un DataFrame, se convierte a DataFrame.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que X sea un DataFrame\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        n_features = X.shape[1]\n",
    "        X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
    "    \n",
    "    # Obtener las predicciones del modelo y aplanarlas\n",
    "    y_pred = model.predict(X)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.ravel()\n",
    "    \n",
    "    pij = []\n",
    "    for col in X.columns:\n",
    "        x_j = X[col]\n",
    "        if x_j.std() == 0:\n",
    "            pij.append(np.nan)\n",
    "        else:\n",
    "            corr = np.corrcoef(y_pred, x_j)[0, 1]\n",
    "            pij.append(corr)\n",
    "    return np.array(pij)\n",
    "\n",
    "def compute_permutation_importance(model, X, y):\n",
    "    \"\"\"\n",
    "    Para modelos complejos, calcula la importancia de cada variable mediante\n",
    "    permutación. Devuelve la media de la importancia en n_repeats.\n",
    "    \"\"\"\n",
    "    result = permutation_importance(model, X, y, scoring=\"r2\", n_repeats=10, random_state=0)\n",
    "    return result.importances_mean\n",
    "\n",
    "def plot_heatmap(matrix, col_labels, row_labels, title, ax=None):\n",
    "    \"\"\"\n",
    "    Dibuja un mapa de calor de la matriz usando seaborn.\n",
    "    'col_labels' son las etiquetas de las columnas y 'row_labels' las de las filas.\n",
    "    Si se especifica 'ax', se dibuja en ese subplot; de lo contrario, crea uno nuevo.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    sns.heatmap(matrix, annot=True, fmt=\".2f\", xticklabels=col_labels,\n",
    "                yticklabels=row_labels, cmap=\"viridis\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ba884-084f-4f49-916c-5de550215df9",
   "metadata": {},
   "source": [
    "### 3.5. Definir y Entrenar los Modelos Subrogados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6425a49-3a6c-4903-9247-3825a61ae233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número óptimo de componentes para modelar PLS es: 9\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Preparamos el modelo PLS calculando sus componentes óptimos\n",
    "# =============================================================================\n",
    "\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Para PLS, se determina el número óptimo de componentes mediante validación cruzada\n",
    "mse_pls = []\n",
    "componentes = np.arange(1, min(len(X.columns), 20))\n",
    "for n in componentes:\n",
    "    pls_temp = PLSRegression(n_components=n)\n",
    "    scores = cross_val_score(pls_temp, X_train_scaled, Y_train_scaled, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_pls.append(-scores.mean())\n",
    "n_componentes_optimos = componentes[np.argmin(mse_pls)]\n",
    "print(f'El número óptimo de componentes para modelar PLS es: {n_componentes_optimos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a353c49f-82ee-4ef1-866b-0259d1980953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Definir el espacio de búsqueda para los hiperparámetros del kernel.\\n# La notación \"kernel__k1__length_scale\" y \"kernel__k2__noise_level\" es la que usa scikit-learn\\n# para acceder a los parámetros del kernel compuesto (k1 corresponde a RBF y k2 a WhiteKernel).\\nparam_space = {\\n    \"kernel__k1__length_scale\": Real(1e-2, 1e3, prior=\"log-uniform\"),\\n    \"kernel__k2__noise_level\": Real(1e-8, 1e+2, prior=\"log-uniform\")\\n}\\n# Configurar la optimización bayesiana con BayesSearchCV\\nopt = BayesSearchCV(\\n    estimator=gpr,\\n    search_spaces=param_space,\\n    n_iter=50,           # número de iteraciones de búsqueda\\n    cv=3,                # validación cruzada de 3 pliegues\\n    scoring=\"neg_mean_squared_error\",\\n    random_state=42,\\n    n_jobs=-1\\n)\\n\\n# Ejecutar la búsqueda sobre los datos escalados\\nopt.fit(X_train, y_train)\\n\\n# Mostrar los mejores hiperparámetros encontrados\\nprint(\"Mejores hiperparámetros encontrados:\")\\nprint(opt.best_params_)\\nprint(\"Mejor score (neg MSE):\", opt.best_score_)\\n\\n# Evaluar el modelo optimizado en el conjunto de test\\ny_pred = opt.predict(X_test)\\nfrom sklearn.metrics import mean_squared_error, r2_score\\nmse = mean_squared_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(f\"MSE en test: {mse:.3e}\")\\nprint(f\"R² en test: {r2:.3f}\")\\n\\n# Puedes también imprimir el kernel final ajustado:\\nprint(\"Kernel final optimizado:\")\\nprint(opt.best_estimator_.kernel_)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# =============================================================================\n",
    "# Hacemos un estudio preliminar (Script completo) para determinar los \n",
    "# hiperparámetros que del modelo Kriging que mejor se adaptan a estos datos.\n",
    "# =============================================================================\n",
    "\n",
    "X = df[features]\n",
    "y = df[outputs]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled  = scaler.fit_transform(y)\n",
    "\n",
    "# Dividir los datos y escalar X (GPR es sensible a la escala)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instanciar el modelo GPR sin reinicios en el optimizador ya que BayesSearchCV se encargará de buscar\n",
    "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "         WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-8, 1e+2))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=0)\n",
    "\n",
    "'''\n",
    "'''\n",
    "# Usamos los parámetros obtenidos en un primer entrenamiento:\n",
    "kernel = RBF(length_scale=1.12, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "         WhiteKernel(noise_level=0.035, noise_level_bounds=(1e-8, 1e+7))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)\n",
    "'''\n",
    "'''\n",
    "# Definir el espacio de búsqueda para los hiperparámetros del kernel.\n",
    "# La notación \"kernel__k1__length_scale\" y \"kernel__k2__noise_level\" es la que usa scikit-learn\n",
    "# para acceder a los parámetros del kernel compuesto (k1 corresponde a RBF y k2 a WhiteKernel).\n",
    "param_space = {\n",
    "    \"kernel__k1__length_scale\": Real(1e-2, 1e3, prior=\"log-uniform\"),\n",
    "    \"kernel__k2__noise_level\": Real(1e-8, 1e+2, prior=\"log-uniform\")\n",
    "}\n",
    "# Configurar la optimización bayesiana con BayesSearchCV\n",
    "opt = BayesSearchCV(\n",
    "    estimator=gpr,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=50,           # número de iteraciones de búsqueda\n",
    "    cv=3,                # validación cruzada de 3 pliegues\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ejecutar la búsqueda sobre los datos escalados\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(opt.best_params_)\n",
    "print(\"Mejor score (neg MSE):\", opt.best_score_)\n",
    "\n",
    "# Evaluar el modelo optimizado en el conjunto de test\n",
    "y_pred = opt.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE en test: {mse:.3e}\")\n",
    "print(f\"R² en test: {r2:.3f}\")\n",
    "\n",
    "# Puedes también imprimir el kernel final ajustado:\n",
    "print(\"Kernel final optimizado:\")\n",
    "print(opt.best_estimator_.kernel_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11d26e94-ad20-4afc-bc20-35c9bda9e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Kriging\n",
    "# Usamos como kernel inicial los hiperparámetros del script anterior:\n",
    "# - RBF(length_scale) con límites en escala logarítmica\n",
    "# - WhiteKernel(noise_level) con límites amplios\n",
    "#==============================================================================\n",
    "kernel = RBF(length_scale=3.74, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "               WhiteKernel(noise_level=0.00211, noise_level_bounds=(1e-8, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "136e73fb-2f98-4e6e-9d78-9924a0165c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Se definen los modelos a utilizar:\n",
    "# - PLSRegression: Modelo de componentes latentes para regresión.\n",
    "# - LinearRegression: Regresión lineal clásica.\n",
    "# - GaussianProcessRegressor (GPR): Modelo de proceso gaussiano (Kriging), que utiliza un kernel (aquí RBF + WhiteKernel).\n",
    "# - SVR: Support Vector Regression, envuelto en MultiOutputRegressor para manejar salidas unidimensionales.\n",
    "# - RandomForestRegressor (RF): Modelo de ensamble basado en árboles (opcional).\n",
    "#==============================================================================\n",
    "models = {\n",
    "    \"PLS\": lambda: PLSRegression(n_components=n_componentes_optimos),\n",
    "    \"LR\": lambda: LinearRegression(),\n",
    "    \"GPR\": lambda: GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10),\n",
    "    \"SVR\": lambda: MultiOutputRegressor(SVR(kernel='rbf')),\n",
    "    \"RF\": lambda: RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "928cebf1-2145-4ecd-a080-48ecd800bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionarios para almacenar resultados (CoP, p_ij y predicciones)\n",
    "cop_results = {output: {} for output in outputs}      # CoP para cada modelo y salida\n",
    "pij_results = {output: {} for output in outputs}       # Parámetros p_ij para cada modelo y salida\n",
    "predictions = {output: {} for output in outputs}        # Predicciones en test para cada modelo y salida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05128a80-ae34-4354-beff-4b5e4fc7187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Se entrenan los modelos por separado.\n",
    "#==============================================================================\n",
    "# --- PLS (componentes optimizados) ---\n",
    "model_PLS = PLSRegression(n_components = n_componentes_optimos)\n",
    "model_PLS.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_pls = model_PLS.predict(X_test_scaled)\n",
    "\n",
    "# --- Regresión lineal (LR) ---\n",
    "model_LR = LinearRegression()\n",
    "model_LR.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_lr = model_LR.predict(X_test_scaled)\n",
    "\n",
    "# --- KRIGING (GPR) ---\n",
    "kernel = RBF(length_scale=3.74, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "               WhiteKernel(noise_level=0.00211, noise_level_bounds=(1e-8, 100))\n",
    "model_kriging = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)\n",
    "model_kriging.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_kriging = model_kriging.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo Support Vector Regression (SVR) --- \n",
    "model_svr = MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "model_svr.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_svr = model_svr.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo Random Forest (RF)---\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_rf = model_rf.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7dae138-fe68-4c12-a581-45c79acbf5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:660: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL: .\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:660: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL: .\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:660: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL: .\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Definir modelos subrogados\n",
    "# =============================================================================\n",
    "# Entrenar cada modelo para cada variable de salida\n",
    "# Se usa X_train_scaled y Y_train_scaled para entrenar modelos sensibles a la escala.\n",
    "# Entrenamiento de modelos para cada variable de salida\n",
    "for output in outputs:\n",
    "    # Extraer la salida (asegurarse de trabajar con datos escalados)\n",
    "    y_train_output = Y_train_scaled[output]\n",
    "    y_test_output  = Y_test_scaled[output]\n",
    "    \n",
    "    # Si la variable de salida es constante, se salta\n",
    "    if float(y_train_output.std()) == 0:\n",
    "        print(f\"Skipping output {output} because it is constant.\")\n",
    "        continue\n",
    "    \n",
    "    for model_name, model_constructor in models.items():\n",
    "        model = model_constructor()\n",
    "        try:\n",
    "            # Para SVR (envuelto en MultiOutputRegressor), la salida se redimensiona a 2D\n",
    "            if model_name == \"SVR\":\n",
    "                model.fit(X_train_scaled, y_train_output.values.reshape(-1, 1))\n",
    "            else:\n",
    "                model.fit(X_train_scaled, y_train_output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name} for {output}: {e}\")\n",
    "            cop_results[output][model_name] = np.nan\n",
    "            pij_results[output][model_name] = np.full(X_train_scaled.shape[1], np.nan)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Predicción en el conjunto de prueba (X_test_scaled ya es DataFrame)\n",
    "            y_pred_test = model.predict(X_test_scaled)\n",
    "            y_pred_test = np.array(y_pred_test).ravel()\n",
    "            cop = compute_CoP(y_test_output, y_pred_test)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing CoP for {model_name} on {output}: {e}\")\n",
    "            cop = np.nan\n",
    "        cop_results[output][model_name] = cop\n",
    "        predictions[output][model_name] = y_pred_test\n",
    "\n",
    "        try:\n",
    "            # Asegurarse de usar un DataFrame para calcular p_ij\n",
    "            # X_train_scaled se creó como DataFrame; pero se refuerza la verificación:\n",
    "            if not isinstance(X_train_scaled, pd.DataFrame):\n",
    "                X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "            else:\n",
    "                X_train_df = X_train_scaled.copy()\n",
    "            pij = compute_pij(model, X_train_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing p_ij for {model_name} on {output}: {e}\")\n",
    "            pij = np.full(X_train_scaled.shape[1], np.nan)\n",
    "        pij_results[output][model_name] = pij"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b06473-a918-4a10-89a8-e82d4fec0aaf",
   "metadata": {},
   "source": [
    "### 3.6. Seleccionar el Mejor Modelo para Cada Salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65f52c56-4819-4f59-af26-d4dfa4743c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejores modelos por variable de salida:\n",
      "  p1::W: GPR (CoP = 0.999)\n",
      "  p4::GFF: GPR (CoP = 0.999)\n",
      "  p5::BSP_T: GPR (CoP = 0.986)\n",
      "  p6::BSP_n: GPR (CoP = 0.940)\n",
      "  p7::BSP_Mu: GPR (CoP = 0.935)\n",
      "  p8::MSP_n: GPR (CoP = 0.890)\n",
      "  p9::UWP_Mu: GPR (CoP = 0.841)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Comparar CoP y elegir el mejor modelo para cada variable de salida\n",
    "# =============================================================================\n",
    "best_models = {}\n",
    "for output in outputs:\n",
    "    cop_dict = cop_results[output]\n",
    "    if len(cop_dict) == 0:\n",
    "        continue\n",
    "    best_model = max(cop_dict, key=lambda m: cop_dict[m] if not np.isnan(cop_dict[m]) else -np.inf)\n",
    "    best_models[output] = best_model\n",
    "\n",
    "print(\"\\nMejores modelos por variable de salida:\")\n",
    "for output in outputs:\n",
    "    best_model = best_models.get(output)\n",
    "    if best_model is None:\n",
    "        print(f\"  {output}: No se evaluó ningún modelo (posiblemente la variable es constante o hubo error)\")\n",
    "    else:\n",
    "        print(f\"  {output}: {best_model} (CoP = {cop_results[output][best_model]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e83efa-1ad7-4760-bcba-8b35ece88215",
   "metadata": {},
   "source": [
    "### 3.7. Visualización de Resultados: CoP y Mapas de Calor de p_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bebf768f-c37d-45b2-a905-e6da98d701fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar gráficamente los CoP de cada modelo para cada salida en subplots\n",
    "# =============================================================================\n",
    "n_out = len(outputs)\n",
    "ncols = 3\n",
    "nrows = ceil(n_out / ncols)\n",
    "fig1, axes1 = plt.subplots(nrows, ncols, figsize=(12, 6*nrows))\n",
    "if n_out == 1:\n",
    "    axes1 = [axes1]\n",
    "else:\n",
    "    axes1 = axes1.flatten()\n",
    "for i, output in enumerate(outputs):\n",
    "    model_names = list(cop_results[output].keys())\n",
    "    cop_vals = [cop_results[output][m] for m in model_names]\n",
    "    ax = axes1[i]\n",
    "    ax.bar(model_names, cop_vals, color=\"steelblue\")\n",
    "    ax.set_title(f\"CoP for {output}\", fontsize=14)\n",
    "    ax.set_ylabel(\"CoP\", fontsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "# Guardar la figura en la carpeta 'Figuras_MOP/(La carpeta que corresponda)'\n",
    "figure_file = os.path.join(figure_path, 'CoP para cada modelo.png')\n",
    "plt.savefig(figure_file, dpi =1080)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02833ddb-438d-4491-a81f-cb8b589cab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar los p_ij en mapas de calor: filas = features, columnas = salidas\n",
    "# Se genera un heatmap para cada modelo evaluado\n",
    "# =============================================================================\n",
    "for model_name in models.keys():\n",
    "    # Construir una matriz de p_ij: filas = features, columnas = outputs\n",
    "    pij_matrix = []\n",
    "    valid_outputs = []\n",
    "    for output in outputs:\n",
    "        if model_name in pij_results[output]:\n",
    "            pij_matrix.append(pij_results[output][model_name])\n",
    "            valid_outputs.append(output)\n",
    "    if len(pij_matrix) == 0:\n",
    "        continue\n",
    "    pij_matrix = np.array(pij_matrix).T  # dimensiones: (n_features, n_outputs)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_heatmap(pij_matrix, col_labels=valid_outputs, row_labels=features,\n",
    "                 title=f\"Mapa de calor de p_ij para {model_name}\", ax=ax)\n",
    "    plt.tight_layout()\n",
    "    # Guardar la figura en la carpeta 'Figuras_MOP/(La carpeta que corresponda)'\n",
    "    figure_file = os.path.join(figure_path, f\"Mapa de calor de p_ij para {model_name}.png\")\n",
    "    plt.savefig(figure_file, dpi =1080)\n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f6b8635-3588-4bd7-b72b-464b0928de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar el mapa de calor de la matriz de correlación de Pearson (entradas vs salidas)\n",
    "# =============================================================================\n",
    "pearson_matrix = np.zeros((len(features), len(outputs)))\n",
    "for i, feat in enumerate(features):\n",
    "    for j, out in enumerate(outputs):\n",
    "        if df[feat].std() == 0 or df[out].std() == 0:\n",
    "            pearson_matrix[i, j] = np.nan\n",
    "        else:\n",
    "            pearson_matrix[i, j] = np.corrcoef(df[feat], df[out])[0, 1]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_heatmap(pearson_matrix, col_labels=outputs, row_labels=features,\n",
    "             title=\"Matriz de correlación de Pearson (entradas vs salidas)\", ax=ax)\n",
    "plt.tight_layout()\n",
    "# Guardar la figura en la carpeta 'Figuras_MOP/(La carpeta que corresponda)'\n",
    "figure_file = os.path.join(figure_path, 'Mapa de calor Pearson.png')\n",
    "plt.savefig(figure_file, dpi =1080)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1fff8-3e4e-404b-982f-87c86044f807",
   "metadata": {},
   "source": [
    "### 3.8. Reentrenar con Todo el Dataset y Generar Predicciones Óptimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d338c90d-80ad-46e3-8fe6-8e91efd22646",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = len(outputs)\n",
    "ncols = 3\n",
    "nrows = ceil(n_out / ncols)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12, 6 * nrows))\n",
    "if n_out == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    best_model_name = best_models.get(output, None)\n",
    "    if best_model_name is None:\n",
    "        print(f\"Sin modelo para {output}\")\n",
    "        continue\n",
    "\n",
    "    # Recupera las predicciones en escala para el mejor modelo en el test set\n",
    "    y_pred_scaled = predictions[output][best_model_name]\n",
    "    \n",
    "    # Desescalar las predicciones usando los parámetros del scaler_Y para esa salida\n",
    "    col_idx = Y.columns.get_loc(output)\n",
    "    y_pred_unscaled = y_pred_scaled * 1\n",
    "    \n",
    "    # Valores reales sin escalar en el test set\n",
    "    y_true_unscaled = Y_test[output].values\n",
    "    \n",
    "    # Filtrar valores válidos (sin NaN)\n",
    "    mask = ~np.isnan(y_true_unscaled) & ~np.isnan(y_pred_unscaled)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    y_true_valid = y_true_unscaled[mask]\n",
    "    y_pred_valid = y_pred_unscaled[mask]\n",
    "    \n",
    "    # Calcular métricas en la escala original\n",
    "    r2 = r2_score(y_true_valid, y_pred_valid)\n",
    "    mse = mean_squared_error(y_true_valid, y_pred_valid)\n",
    "    cop = compute_CoP(y_true_valid, y_pred_valid)\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.scatter(y_true_valid, y_pred_valid, alpha=0.6, edgecolor=\"k\")\n",
    "    ax.plot([min(y_true_valid), max(y_true_valid)],\n",
    "            [min(y_true_valid), max(y_true_valid)], 'r--', lw=2)\n",
    "    ax.set_xlabel(\"FEA Simulation\", fontsize=12)\n",
    "    ax.set_ylabel(\"Surrogate Prediction\", fontsize=12)\n",
    "    ax.set_title(f\"{output}\\nR²={r2:.3f}, CoP={cop:.3f}, MSE={mse:.3e}\", fontsize=14)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2511ad93-aff4-465a-a050-17bac878f2d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimal_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     y_pred_valid \u001b[38;5;241m=\u001b[39m y_pred_unscaled[mask]\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Guardamos la predicción final (ya desescalada)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     optimal_predictions[output] \u001b[38;5;241m=\u001b[39m y_pred_valid\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Creamos un DataFrame con las predicciones óptimas (ya en la escala original)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimal_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for i, output in enumerate(outputs):\n",
    "    best_model_name = best_models.get(output, None)\n",
    "    if best_model_name is None:\n",
    "        print(f\"Sin modelo para {output}\")\n",
    "        continue\n",
    "\n",
    "    # Recupera las predicciones en escala para el mejor modelo en el test set\n",
    "    y_pred_scaled = predictions[output][best_model_name]\n",
    "    \n",
    "    # Desescalar las predicciones usando los parámetros del scaler_Y para esa salida\n",
    "    col_idx = Y.columns.get_loc(output)\n",
    "    y_pred_unscaled = y_pred_scaled * scaler_Y.scale_[col_idx] + scaler_Y.mean_[col_idx]\n",
    "    \n",
    "    # Valores reales sin escalar en el test set\n",
    "    y_true_unscaled = Y_test[output].values\n",
    "    \n",
    "    # Filtrar valores válidos (sin NaN)\n",
    "    mask = ~np.isnan(y_true_unscaled) & ~np.isnan(y_pred_unscaled)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    y_true_valid = y_true_unscaled[mask]\n",
    "    y_pred_valid = y_pred_unscaled[mask]\n",
    "\n",
    "    # Guardamos la predicción final (ya desescalada)\n",
    "    optimal_predictions[output] = y_pred_valid\n",
    "    print()\n",
    "\n",
    "# Creamos un DataFrame con las predicciones óptimas (ya en la escala original)\n",
    "df_optimal = X.copy()\n",
    "for output in outputs:\n",
    "    if output in optimal_predictions:\n",
    "        df_optimal[f\"{output}_pred\"] = optimal_predictions[output]\n",
    "    else:\n",
    "        df_optimal[f\"{output}_pred\"] = np.nan\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37f291-195e-48ee-85bc-294b28b52fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Escalamos la totalidad de X e Y\n",
    "X_full_scaled = scaler_X.transform(X)  # array numpy\n",
    "Y_full_scaled = scaler_Y.transform(Y)  # array numpy\n",
    "\n",
    "# Para comodidad, los convertimos en DataFrames con nombres de columnas\n",
    "X_full_scaled_df = pd.DataFrame(X_full_scaled, columns=X.columns)\n",
    "Y_full_scaled_df = pd.DataFrame(Y_full_scaled, columns=Y.columns)\n",
    "\n",
    "# =============================================================================\n",
    "# Con el mejor modelo para cada salida, reentrenar con todo el dataset y generar predicciones óptimas\n",
    "# =============================================================================\n",
    "optimal_predictions = {}\n",
    "for output in outputs:\n",
    "    if output not in best_models:\n",
    "        continue\n",
    "    model_name = best_models[output]\n",
    "    model = models[model_name]() # creamos una nueva instancia\n",
    "\n",
    "    # Tomamos la columna correspondiente (escalada) para entrenar\n",
    "    y_full_scaled = Y_full_scaled_df[output].values\n",
    "\n",
    "    '''\n",
    "    # Entrenamos con X_full_scaled_df y la columna Y escalada\n",
    "    if model_name == \"SVR\":\n",
    "        # SVR (dentro de MultiOutputRegressor) pide shape 2D para la Y\n",
    "        model.fit(X_full_scaled_df, y_full_scaled.reshape(-1, 1))\n",
    "        y_pred_scaled = model.predict(X_full_scaled_df).ravel()\n",
    "    else:\n",
    "        model.fit(X_full_scaled_df, y_full_scaled)\n",
    "        y_pred_scaled = model.predict(X_full_scaled_df)\n",
    "    '''\n",
    "\n",
    "    y_pred_scaled = Y_full_scaled_df[output].values\n",
    "    \n",
    "    # Desescalar las predicciones\n",
    "    # Ojo: scaler_Y.inverse_transform pide 2D; luego hacemos .ravel() si necesitamos 1D\n",
    "    # Desescalar las predicciones manualmente para la salida actual\n",
    "    col_idx = Y.columns.get_loc(output)\n",
    "    y_pred_full = y_pred_scaled * scaler_Y.scale_[col_idx] + scaler_Y.mean_[col_idx]\n",
    "    \n",
    "    # Guardamos la predicción final (ya desescalada)\n",
    "    optimal_predictions[output] = y_pred_full\n",
    "\n",
    "# Creamos un DataFrame con las predicciones óptimas (ya en la escala original)\n",
    "df_optimal = X.copy()\n",
    "for output in outputs:\n",
    "    if output in optimal_predictions:\n",
    "        df_optimal[f\"{output}_pred\"] = optimal_predictions[output]\n",
    "    else:\n",
    "        df_optimal[f\"{output}_pred\"] = np.nan\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41fb660-8d0d-4282-bd07-31e74573796a",
   "metadata": {},
   "source": [
    "### 3.9. Visualización Final: Comparación entre Valores FEA y Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b617bcc-91ae-4b21-b1aa-c2bee95ae9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Subplots para comparar, para cada salida, los valores FEA vs. las predicciones óptimas.\n",
    "# Se filtran filas con NaN antes de calcular las métricas.\n",
    "# =============================================================================\n",
    "fig2, axes2 = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 6*nrows))\n",
    "if n_out == 1:\n",
    "    axes2 = [axes2]\n",
    "else:\n",
    "    axes2 = axes2.flatten()\n",
    "for i, output in enumerate(outputs):\n",
    "    y_true = df[output].values # valores originales\n",
    "    y_pred = df_optimal[f\"{output}_pred\"].values\n",
    "    \n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"No valid data for metrics evaluation of {output}.\")\n",
    "        continue\n",
    "        \n",
    "    y_true_valid = y_true[mask]\n",
    "    y_pred_valid = y_pred[mask]\n",
    "    \n",
    "    r2 = r2_score(y_true_valid, y_pred_valid)\n",
    "    mse = mean_squared_error(y_true_valid, y_pred_valid)\n",
    "\n",
    "    # CoP lo calculamos si lo tienes definido (asumiendo tu compute_CoP)\n",
    "    cop_full = compute_CoP(y_true_valid, y_pred_valid)\n",
    "    \n",
    "    ax = axes2[i]\n",
    "    ax.scatter(y_true_valid, y_pred_valid, alpha=0.6, edgecolor=\"k\")\n",
    "    ax.plot([y_true_valid.min(), y_true_valid.max()],\n",
    "            [y_true_valid.min(), y_true_valid.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel(\"FEA Simulation\", fontsize=12)\n",
    "    ax.set_ylabel(\"Surrogate Prediction\", fontsize=12)\n",
    "    ax.set_title(\n",
    "        f\"{output}\\nR²={r2:.3f}, CoP={cop_full:.3f}, MSE={mse:.3e}\",\n",
    "        fontsize=14\n",
    "    )\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    \n",
    "plt.tight_layout()\n",
    "# Guardar la figura en la carpeta 'Figuras_MOP/(La carpeta que corresponda)'\n",
    "figure_file = os.path.join(figure_path, 'los valores FEA vs. las predicciones óptimas.png')\n",
    "plt.savefig(figure_file, dpi =1080)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f897a-62cc-4402-9a02-3b3fac49ae98",
   "metadata": {},
   "source": [
    "### 3.10. Guardar el Nuevo Dataset con Predicciones Óptimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651b52f-57eb-469e-8da9-a04fe06537f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el nuevo dataset\n",
    "model_file = os.path.join(modelo_path, \"trained_database_optimal.csv\")\n",
    "df_optimal.to_csv(model_file, index=False)\n",
    "print(\"\\nNuevo dataset generado: 'trained_database_optimal.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080fd1e-f123-418a-8293-0285b1d3e1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
