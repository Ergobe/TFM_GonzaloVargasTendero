{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe6a29b-9eb9-4744-aa32-544f60294565",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae3baa-03f5-4257-bd0d-b0aad83e2ea2",
   "metadata": {},
   "source": [
    "## 3. Modelo de Optimización y Prognosis (MOP)\n",
    "\n",
    "### 3.1. Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff92533-1e76-4f8f-9cf7-d0d175ed192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paso 0: Importar librerías y definir funciones auxiliares\n",
    "# =============================================================================\n",
    "\n",
    "# Librerías necesarias\n",
    "import os\n",
    "import re  # Import the regular expression module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Preprocesamiento, modelado y métricas\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e4a49d-cf1f-4808-95bb-7162184e43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar nombres de archivo inválidos\n",
    "def clean_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)\n",
    "\n",
    "# Función para calcular la correlación p_ij según la fórmula del paper:\n",
    "#   p_ij = (1/(N-1)) * Σ[(ŷ(k) - μ_ŷ) * (x_j(k) - μ_xj)] / (σ_ŷ σ_xj)\n",
    "def compute_corr(y, x):\n",
    "    N = len(y)\n",
    "    mean_y = np.mean(y)\n",
    "    mean_x = np.mean(x)\n",
    "    std_y = np.std(y, ddof=1)\n",
    "    std_x = np.std(x, ddof=1)\n",
    "    return np.sum((y - mean_y) * (x - mean_x)) / ((N - 1) * std_y * std_x)\n",
    "\n",
    "# Función para dibujar un mapa de calor con seaborn\n",
    "def plot_heatmap(matrix, col_labels, row_labels, title, ax=None):\n",
    "    \"\"\"\n",
    "    Dibuja un mapa de calor de 'matrix' usando seaborn.\n",
    "    'col_labels' y 'row_labels' definen las etiquetas de columnas y filas.\n",
    "    Si se proporciona 'ax', se dibuja en ese subplot; de lo contrario, se crea uno nuevo.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    sns.heatmap(matrix, annot=True, fmt=\".2f\", xticklabels=col_labels,\n",
    "                yticklabels=row_labels, cmap=\"viridis\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "# Función auxiliar para calcular el Coefficient of Prognosis (CoP)\n",
    "def compute_CoP(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    mean_y = np.mean(y_true)\n",
    "    mean_y_pred = np.mean(y_pred)\n",
    "    std_y = np.std(y_true, ddof=1)\n",
    "    std_y_pred = np.std(y_pred, ddof=1)\n",
    "    denominator = (N - 1) * std_y * std_y_pred\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return (np.sum((y_true - mean_y) * (y_pred - mean_y_pred)) / denominator) ** 2\n",
    "\n",
    "# Función para calcular el diccionario de CoP para cada salida y cada modelo\n",
    "def compute_cop_results(metricas, outputs):\n",
    "    \"\"\"\n",
    "    Genera un diccionario de CoP con la estructura:\n",
    "      { output1: { 'PLS': cop_value, 'LR': cop_value, ... },\n",
    "        output2: { 'PLS': cop_value, ... },\n",
    "        ... }\n",
    "    Se asume que 'metricas' tiene, para cada modelo, una lista de valores de CoP\n",
    "    en el mismo orden que 'outputs'.\n",
    "    \"\"\"\n",
    "    cop_results = {}\n",
    "    for j, output in enumerate(outputs):\n",
    "        cop_results[output] = {}\n",
    "        for model_name in metricas.keys():\n",
    "            cop_results[output][model_name] = metricas[model_name]['CoP'][j]\n",
    "    return cop_results\n",
    "\n",
    "# Función para graficar los CoP en subplots y guardar la figura\n",
    "def plot_cop_subplots(cop_results, outputs, figure_path, filename=\"CoP_para_cada_modelo.png\"):\n",
    "    \"\"\"\n",
    "    Dibuja un gráfico de subplots, donde cada subplot es un gráfico de barras con los CoP\n",
    "    de cada modelo para una variable de salida.\n",
    "    La figura se guarda en 'figure_path/filename'.\n",
    "    \"\"\"\n",
    "    n_out = len(outputs)\n",
    "    ncols = 3\n",
    "    nrows = ceil(n_out / ncols)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 6 * nrows))\n",
    "    axes = axes.flatten() if n_out > 1 else [axes]\n",
    "    \n",
    "    for i, output in enumerate(outputs):\n",
    "        model_names = list(cop_results[output].keys())\n",
    "        cop_vals = [cop_results[output][m] for m in model_names]\n",
    "        ax = axes[i]\n",
    "        ax.bar(model_names, cop_vals, color=\"steelblue\")\n",
    "        ax.set_title(f\"CoP para {output}\", fontsize=14)\n",
    "        ax.set_ylabel(\"CoP\", fontsize=12)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    \n",
    "    # Eliminar ejes sobrantes si existen\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    figure_file = os.path.join(figure_path, filename)\n",
    "    plt.savefig(figure_file, dpi=1080)\n",
    "    plt.close()\n",
    "    print(f\"Figura de CoP guardada en: {figure_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b369df5f-7f54-4a94-8e28-e7a51fc84316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta de datos: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\DB_MOP\\design_DB_preprocessed_400_Optimizado.csv\n",
      "Ruta de figuras: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Figuras_MOP\\400_MOT_Optimizado\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\400_MOT_Optimizado\n",
      "Ruta de modelos: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\400_MOT_Optimizado\n",
      "Archivo cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 1: Definir rutas, cargar datos y configurar directorios\n",
    "# =============================================================================\n",
    "base_path = os.getcwd()  # Se asume que el notebook se ejecuta desde la carpeta 'MOP'\n",
    "db_path = os.path.join(base_path, \"DB_MOP\")\n",
    "fig_path = os.path.join(base_path, \"Figuras_MOP\")\n",
    "model_path = os.path.join(base_path, \"Modelos_MOP\")\n",
    "\n",
    "# Ruta al archivo de la base de datos\n",
    "data_file = os.path.join(db_path, \"design_DB_preprocessed_400_Optimizado.csv\")\n",
    "print(\"Ruta de datos:\", data_file)\n",
    "\n",
    "# Ruta donde se guardarán las figuras\n",
    "figure_path = os.path.join(fig_path, \"400_MOT_Optimizado\")\n",
    "if not os.path.exists(figure_path):\n",
    "    os.makedirs(figure_path)\n",
    "print(\"Ruta de figuras:\", figure_path)\n",
    "\n",
    "# Ruta al archivo de los modelos\n",
    "model_path = os.path.join(model_path, \"400_MOT_Optimizado\")\n",
    "print(model_path)\n",
    "print(\"Ruta de modelos:\", model_path)\n",
    "\n",
    "# Lectura del archivo CSV\n",
    "try:\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(\"Archivo cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado. Revisa la ruta del archivo.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Problema al analizar el archivo CSV. Revisa el formato del archivo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado: {e}\")\n",
    "\n",
    "# Función para limpiar nombres de archivo inválidos\n",
    "def clean_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba34d7a-68ea-4aff-9af0-ec390f9e1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paso 2: Preprocesar datos: separar columnas en X, M, P y convertir a numérico\n",
    "# =============================================================================\n",
    "X_cols = [col for col in df.columns if col.startswith('x')]\n",
    "M_cols = [col for col in df.columns if col.startswith('m')]\n",
    "P_cols = [col for col in df.columns if col.startswith('p')]\n",
    "\n",
    "X = df[X_cols].copy()\n",
    "M = df[M_cols].copy()\n",
    "P = df[P_cols].copy()\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "for col in M.columns:\n",
    "    M[col] = pd.to_numeric(M[col], errors='coerce')\n",
    "for col in P.columns:\n",
    "    P[col] = pd.to_numeric(P[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62821794-548b-465e-98a6-2c35a1fc33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de entrada: ['x1::OSD', 'x2::Dint', 'x3::L', 'x4::tm', 'x5::hs2', 'x6::wt', 'x7::Nt', 'x8::Nh']\n",
      "Variables de entrada: ['x1::OSD', 'x2::Dint', 'x3::L', 'x4::tm', 'x5::hs2', 'x6::wt', 'x7::Nt', 'x8::Nh', 'm1::Drot', 'm2::Dsh', 'm3::he', 'm4::Rmag', 'm5::Rs', 'm6::GFF']\n",
      "Variables de salida: ['p1::W', 'p4::GFF', 'p5::BSP_T', 'p6::BSP_n', 'p7::BSP_Mu', 'p8::MSP_n', 'p9::UWP_Mu']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 3: Seleccionar variables de entrada y salida\n",
    "# =============================================================================\n",
    "# Las variables de salida se toman de P; se eliminan 'p2::Tnom' y 'p3::nnom' si existen.\n",
    "outputs = [col for col in P.columns]\n",
    "if 'p2::Tnom' in outputs:\n",
    "    outputs.remove('p2::Tnom')\n",
    "if 'p3::nnom' in outputs:\n",
    "    outputs.remove('p3::nnom')\n",
    "\n",
    "# Las variables de entrada se obtienen concatenando X y M.\n",
    "X_M = pd.concat([X, M], axis=1)\n",
    "features = list(X.columns)\n",
    "all_features = list(X_M.columns)\n",
    "print(\"Variables de entrada:\", features)\n",
    "print(\"Variables de entrada:\", all_features)\n",
    "print(\"Variables de salida:\", outputs)\n",
    "\n",
    "# Redefinir X y Y usando los nombres de columnas seleccionados\n",
    "X = df[features]\n",
    "Y = df[outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c863f8dd-22be-42ac-9241-fa530feee6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paso 4: Escalado de datos y separación en entrenamiento/test\n",
    "# =============================================================================\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y)\n",
    "\n",
    "# Separar en conjuntos de entrenamiento (80%) y test (20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train_scaled = pd.DataFrame(scaler_X.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled  = pd.DataFrame(scaler_X.transform(X_test), columns=X_test.columns)\n",
    "Y_train_scaled = pd.DataFrame(scaler_Y.transform(Y_train), columns=Y_train.columns)\n",
    "Y_test_scaled  = pd.DataFrame(scaler_Y.transform(Y_test), columns=Y_test.columns)\n",
    "\n",
    "# Crear DataFrames escalados completos (para reentrenamiento final y predicciones)\n",
    "X_scaled_df = pd.DataFrame(scaler_X.transform(X), columns=X.columns, index=X.index)\n",
    "Y_scaled_df = pd.DataFrame(scaler_Y.transform(Y), columns=Y.columns, index=Y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a32dba-7e47-4299-9326-259d0cab5ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optimización de PLS ===\n",
      "El número óptimo de componentes para modelar PLS es: 7\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 5: Cálculo de los hiperparámetros para cada modelo\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# Explicación:\n",
    "# =============================================================================\n",
    "# Es necesario reentrenar estos modelos con los hiperparámetros óptimos obtenidos mediante\n",
    "# la optimización (por ejemplo, mediante GridSearchCV) porque:\n",
    "#\n",
    "# 1. Los modelos iniciales fueron entrenados con parámetros predefinidos o configuraciones preliminares.\n",
    "# 2. La optimización (por ejemplo, GridSearchCV) utiliza validación cruzada para determinar la\n",
    "#    configuración de hiperparámetros que minimiza el error (o maximiza una métrica) en datos de\n",
    "#    validación, mejorando así la capacidad del modelo para generalizar.\n",
    "# 3. Una vez identificados los mejores parámetros, reentrenar el modelo usando estos parámetros\n",
    "#    garantiza que el modelo final se construya con la configuración óptima, lo cual suele traducirse\n",
    "#    en un desempeño superior en datos no vistos.\n",
    "#\n",
    "# Por ello, se generan los modelos optimizados:\n",
    "# - model_PLS_opt: PLSRegression reentrenado con el mejor n_components.\n",
    "# - model_LR_opt: LinearRegression reentrenado con el mejor valor de fit_intercept.\n",
    "# - model_SVR_opt: MultiOutputRegressor(SVR(...)) reentrenado con los mejores parámetros C y epsilon.\n",
    "# - model_RF_opt: RandomForestRegressor reentrenado con los mejores valores para n_estimators, max_depth y min_samples_split.\n",
    "# - model_GPR_opt: GaussianProcessRegressor reentrenado con el kernel optimizado (con final_length_scale y final_noise_level).\n",
    "#\n",
    "# Estos modelos optimizados estarán listos para usarse en predicción y evaluación.\n",
    "\n",
    "# =============================================================================\n",
    "# Paso 5.1: PLS - Hiperparámetros\n",
    "# =============================================================================\n",
    "\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Para PLS, se determina el número óptimo de componentes mediante validación cruzada\n",
    "mse_pls = []\n",
    "componentes = np.arange(1, min(len(X.columns), 20))\n",
    "for n in componentes:\n",
    "    pls_temp = PLSRegression(n_components=n)\n",
    "    scores = cross_val_score(pls_temp, X_train_scaled, Y_train_scaled, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_pls.append(-scores.mean())\n",
    "n_componentes_optimos = componentes[np.argmin(mse_pls)]\n",
    "print(\"=== Optimización de PLS ===\")\n",
    "print(f'El número óptimo de componentes para modelar PLS es: {n_componentes_optimos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f756fb-d33a-4e99-9704-641fd03ecad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optimización de GPR ===\n",
      "Mejores hiperparámetros encontrados:\n",
      "OrderedDict({'kernel__k1__length_scale': 5.501270802563586, 'kernel__k2__noise_level': 0.06028286397265279})\n",
      "Mejor score (neg MSE): -0.08476939154282176\n",
      "MSE en test: 7.113e-02\n",
      "R² en test: 0.926\n",
      "Kernel final optimizado:\n",
      "RBF(length_scale=2.91) + WhiteKernel(noise_level=0.019)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 5.2: Kriging (GPR) - Hiperparámetros\n",
    "# =============================================================================\n",
    "# Instanciar el modelo GPR sin reinicios en el optimizador ya que BayesSearchCV se encargará de buscar\n",
    "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "         WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-8, 1e+2))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=0)\n",
    "\n",
    "'''\n",
    "# Usamos los parámetros obtenidos en un primer entrenamiento:\n",
    "kernel = RBF(length_scale=1.12, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "         WhiteKernel(noise_level=0.035, noise_level_bounds=(1e-8, 1e+7))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)\n",
    "'''\n",
    "\n",
    "# Definir el espacio de búsqueda para los hiperparámetros del kernel.\n",
    "# La notación \"kernel__k1__length_scale\" y \"kernel__k2__noise_level\" es la que usa scikit-learn\n",
    "# para acceder a los parámetros del kernel compuesto (k1 corresponde a RBF y k2 a WhiteKernel).\n",
    "param_space = {\n",
    "    \"kernel__k1__length_scale\": Real(1e-2, 1e3, prior=\"log-uniform\"),\n",
    "    \"kernel__k2__noise_level\": Real(1e-8, 1e+2, prior=\"log-uniform\")\n",
    "}\n",
    "# Configurar la optimización bayesiana con BayesSearchCV\n",
    "opt = BayesSearchCV(\n",
    "    estimator=gpr,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=50,           # número de iteraciones de búsqueda\n",
    "    cv=3,                # validación cruzada de 3 pliegues\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ejecutar la búsqueda sobre los datos escalados\n",
    "opt.fit(X_train_scaled, Y_train_scaled)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"=== Optimización de GPR ===\")\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(opt.best_params_)\n",
    "print(\"Mejor score (neg MSE):\", opt.best_score_)\n",
    "\n",
    "# Suponiendo que 'opt' es tu objeto BayesSearchCV que ha sido ajustado\n",
    "best_gpr = opt.best_estimator_  # Obtenemos el mejor modelo GPR optimizado\n",
    "final_kernel = best_gpr.kernel_\n",
    "\n",
    "# Asumiendo que el kernel es la suma de RBF (k1) y WhiteKernel (k2)\n",
    "final_length_scale = final_kernel.k1.length_scale\n",
    "final_noise_level = final_kernel.k2.noise_level\n",
    "\n",
    "# Evaluar el modelo optimizado en el conjunto de test\n",
    "y_pred = opt.predict(X_test_scaled)\n",
    "mse = mean_squared_error(Y_test_scaled, y_pred)\n",
    "r2 = r2_score(Y_test_scaled, y_pred)\n",
    "print(f\"MSE en test: {mse:.3e}\")\n",
    "print(f\"R² en test: {r2:.3f}\")\n",
    "\n",
    "# Puedes también imprimir el kernel final ajustado:\n",
    "print(\"Kernel final optimizado:\")\n",
    "print(opt.best_estimator_.kernel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f2e92c-a41c-4dc7-9f2f-ba06869267a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optimización de LR ===\n",
      "Mejores parámetros: {'fit_intercept': False}\n",
      "Mejor score (neg MSE): -0.24818079780553584\n",
      "Hiperparámetros óptimos de LR: {'fit_intercept': False}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 5.3: Regresión Lineal (LR) - Hiperparámetros\n",
    "# =============================================================================\n",
    "# Para LinearRegression, se optimiza el parámetro \"fit_intercept\"\n",
    "lr_param_grid = {\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "lr_grid = GridSearchCV(LinearRegression(), lr_param_grid, \n",
    "                         cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "lr_grid.fit(X_train_scaled, Y_train_scaled)\n",
    "print(\"=== Optimización de LR ===\")\n",
    "print(\"Mejores parámetros:\", lr_grid.best_params_)\n",
    "print(\"Mejor score (neg MSE):\", lr_grid.best_score_)\n",
    "best_lr_params = lr_grid.best_params_  # Ej: {'fit_intercept': True}\n",
    "print(\"Hiperparámetros óptimos de LR:\", best_lr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610734c5-9d63-4323-bbc5-6ee54de11644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optimización de SVR ===\n",
      "Mejores parámetros: {'estimator__C': 10, 'estimator__epsilon': 0.01}\n",
      "Mejor score (neg MSE): -0.09813432416789417\n",
      "Hiperparámetros óptimos de SVR (con prefijo): {'estimator__C': 10, 'estimator__epsilon': 0.01}\n",
      "Hiperparámetros óptimos de SVR: {'C': 10, 'epsilon': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 5.4: Support Vector Regression (SVR) - Hiperparámetros\n",
    "# =============================================================================\n",
    "# Usamos MultiOutputRegressor para manejar salidas múltiples.\n",
    "# Se optimizan los parámetros C y epsilon.\n",
    "svr_param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10, 100],\n",
    "    'estimator__epsilon': [0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "svr_grid = GridSearchCV(MultiOutputRegressor(SVR(kernel='rbf')),\n",
    "                        svr_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "svr_grid.fit(X_train_scaled, Y_train_scaled)\n",
    "print(\"=== Optimización de SVR ===\")\n",
    "print(\"Mejores parámetros:\", svr_grid.best_params_)\n",
    "print(\"Mejor score (neg MSE):\", svr_grid.best_score_)\n",
    "\n",
    "best_svr_params = svr_grid.best_params_\n",
    "print(\"Hiperparámetros óptimos de SVR (con prefijo):\", best_svr_params)\n",
    "\n",
    "# El grid de SVR se realizó usando MultiOutputRegressor, por lo que los parámetros tienen el prefijo 'estimator__'.\n",
    "# Extraemos los parámetros y removemos dicho prefijo para pasarlos al estimador SVR.\n",
    "svr_params = {key.replace('estimator__', ''): value for key, value in best_svr_params.items()}\n",
    "print(\"Hiperparámetros óptimos de SVR:\", svr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdd6fca4-0a42-46f0-b558-e8ce6dab1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optimización de RF ===\n",
      "Mejores parámetros: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Mejor score (neg MSE): -0.27290780366288053\n",
      "Hiperparámetros óptimos de RF: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 5.5: Random Forest (RF) - Hiperparámetros\n",
    "# =============================================================================\n",
    "# Se optimizan n_estimators, max_depth y min_samples_split.\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10, 20]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid,\n",
    "                       cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rf_grid.fit(X_train_scaled, Y_train_scaled)\n",
    "print(\"=== Optimización de RF ===\")\n",
    "print(\"Mejores parámetros:\", rf_grid.best_params_)\n",
    "print(\"Mejor score (neg MSE):\", rf_grid.best_score_)\n",
    "best_rf_params = rf_grid.best_params_\n",
    "print(\"Hiperparámetros óptimos de RF:\", best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80a77520-2022-493c-a8f8-9fa26f82d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paso 6: Entrenar los modelos subrogados\n",
    "# =============================================================================\n",
    "# --- Modelo PLS (con 9 componentes) ---\n",
    "model_PLS = PLSRegression(n_components=n_componentes_optimos)\n",
    "model_PLS.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_pls = model_PLS.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo de Regresión Lineal (LR) ---\n",
    "model_LR = LinearRegression(**best_lr_params)\n",
    "model_LR.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_lr = model_LR.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo KRIGING (GPR) ---\n",
    "kernel = RBF(length_scale=final_length_scale, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "         WhiteKernel(noise_level=final_noise_level, noise_level_bounds=(1e-8, 100))\n",
    "model_kriging = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)\n",
    "model_kriging.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_kriging = model_kriging.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo Support Vector Regression (SVR) ---\n",
    "model_svr = MultiOutputRegressor(SVR(kernel='rbf', **svr_params))\n",
    "model_svr.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_svr = model_svr.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo Random Forest (RF) ---\n",
    "model_rf = RandomForestRegressor(random_state=42, **best_rf_params)\n",
    "model_rf.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_rf = model_rf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6a5872-7ade-422d-b968-22376ccc6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paso 7: Calcular métricas (MSE, RMSE, MAE, R2, CoP) para cada modelo\n",
    "# =============================================================================\n",
    "# Crear un diccionario que asocie cada modelo con sus predicciones sobre el conjunto de test\n",
    "modelos = {\n",
    "    'PLS': (model_PLS, predicciones_test_pls),\n",
    "    'LR': (model_LR, predicciones_test_lr),\n",
    "    'GPR': (model_kriging, predicciones_test_kriging),\n",
    "    'SVR': (model_svr, predicciones_test_svr),\n",
    "    'RF': (model_rf, predicciones_test_rf)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9ae659a-eafe-4c3b-8b9d-ad9792c8d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar diccionario de métricas para cada modelo\n",
    "metricas = {nombre: {} for nombre in modelos.keys()}\n",
    "for nombre, (modelo, preds) in modelos.items():\n",
    "    metricas[nombre]['MSE'] = []\n",
    "    metricas[nombre]['RMSE'] = []\n",
    "    metricas[nombre]['MAE'] = []\n",
    "    metricas[nombre]['R2'] = []\n",
    "    metricas[nombre]['CoP'] = []\n",
    "    # Calcular métricas para cada variable de salida\n",
    "    for i, col in enumerate(Y_test_scaled.columns):\n",
    "        y_true = Y_test_scaled[col].values\n",
    "        y_pred = preds[:, i]\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        cop = compute_CoP(y_true, y_pred)\n",
    "        metricas[nombre]['MSE'].append(mse)\n",
    "        metricas[nombre]['RMSE'].append(rmse)\n",
    "        metricas[nombre]['MAE'].append(mae)\n",
    "        metricas[nombre]['R2'].append(r2)\n",
    "        metricas[nombre]['CoP'].append(cop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363d3a34-8266-435d-8580-cd3f0e6f5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paso 7.1: Representar métricas como gráficos de barras (subplot por métrica) para cada variable de salida\n",
    "# =============================================================================\n",
    "\n",
    "# Lista de métricas a representar\n",
    "metric_names = ['CoP', 'R2', 'MSE', 'RMSE', 'MAE']\n",
    "# Lista de modelos, extraída del diccionario 'metricas'\n",
    "models_list = list(metricas.keys())\n",
    "n_models = len(models_list)\n",
    "\n",
    "# Usar una paleta de colores pastel (una por cada modelo)\n",
    "colors = sns.color_palette(\"pastel\", n_models)\n",
    "\n",
    "# Iterar sobre cada variable de salida\n",
    "for i, output in enumerate(outputs):\n",
    "    # Crear una figura para la variable de salida actual\n",
    "    fig, axs = plt.subplots(1, len(metric_names), figsize=(5 * len(metric_names), 4))\n",
    "    \n",
    "    # Asegurar que axs sea iterable (si solo hay un subplot)\n",
    "    if len(metric_names) == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    # Para cada métrica, crear un subplot con barras para cada modelo\n",
    "    for j, metric in enumerate(metric_names):\n",
    "        # Extraer los valores de la métrica 'metric' para el modelo actual y la variable 'output'\n",
    "        values = [metricas[model][metric][i] for model in models_list]\n",
    "        ax = axs[j]\n",
    "        # Crear gráfico de barras: eje x = modelos, eje y = valor de la métrica\n",
    "        ax.bar(models_list, values, color=colors)\n",
    "        # Configurar título y etiquetas del subplot\n",
    "        ax.set_title(f\"{metric}\", fontsize=10)\n",
    "        ax.set_ylabel(metric, fontsize=9)\n",
    "        # Agregar etiquetas de valor sobre cada barra\n",
    "        for k, v in enumerate(values):\n",
    "            ax.text(k, v, f\"{v:.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "        # Ajustar el límite del eje y para mayor claridad\n",
    "        ax.set_ylim(0, max(values) * 1.1)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    \n",
    "    # Título general de la figura para la variable de salida\n",
    "    fig.suptitle(f\"Métricas para la variable de salida: {output}\", fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Guardar la figura en la carpeta 'figure_path' usando el nombre de la variable de salida\n",
    "    output_file = os.path.join(figure_path, f\"Métricas_{clean_filename(output)}.png\")\n",
    "    plt.savefig(output_file, dpi=1080)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc75b970-48ba-43c6-9ad9-b0b44a8d48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de CoP:\n",
      "                 PLS        LR       GPR       SVR        RF\n",
      "p1::W       0.979490  0.978716  0.985182  0.966342  0.890739\n",
      "p4::GFF     0.696623  0.813672  0.890446  0.778073  0.304846\n",
      "p5::BSP_T   0.783004  0.789361  0.974799  0.941957  0.797822\n",
      "p6::BSP_n   0.863841  0.860485  0.914542  0.880817  0.907121\n",
      "p7::BSP_Mu  0.587415  0.589781  0.880732  0.870167  0.671832\n",
      "p8::MSP_n   0.728010  0.716634  0.947756  0.892229  0.915379\n",
      "p9::UWP_Mu  0.467552  0.468373  0.911622  0.873242  0.692427\n",
      "Figura de CoP guardada en: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Figuras_MOP\\400_MOT_Optimizado\\CoP_para_cada_modelo.png\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 8: Representar gráficamente los CoP para cada modelo y salida\n",
    "# =============================================================================\n",
    "# Crear un DataFrame donde las filas son las variables de salida y las columnas los modelos\n",
    "cop_df = pd.DataFrame({nombre: metricas[nombre]['CoP'] for nombre in modelos.keys()},\n",
    "                      index=Y_test_scaled.columns)\n",
    "print(\"Valores de CoP:\")\n",
    "print(cop_df)\n",
    "\n",
    "# Graficar los CoP con un gráfico de barras\n",
    "cop_df.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Coefficient of Prognosis (CoP) por modelo y variable de salida\")\n",
    "plt.ylabel(\"CoP\")\n",
    "plt.xlabel(\"Variable de salida\")\n",
    "plt.legend(title=\"Modelo\")\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "# Calcular el diccionario cop_results para uso en subplots\n",
    "cop_results = compute_cop_results(metricas, Y_test_scaled.columns.tolist())\n",
    "\n",
    "# Graficar los CoP en subplots y guardar la figura\n",
    "plot_cop_subplots(cop_results, Y_test_scaled.columns.tolist(), figure_path, filename=\"CoP_para_cada_modelo.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "386b289e-5231-476b-ba32-a42f85d985ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo para cada variable de salida basado en CoP:\n",
      " - p1::W: GPR (CoP = 0.985)\n",
      " - p4::GFF: GPR (CoP = 0.890)\n",
      " - p5::BSP_T: GPR (CoP = 0.975)\n",
      " - p6::BSP_n: GPR (CoP = 0.915)\n",
      " - p7::BSP_Mu: GPR (CoP = 0.881)\n",
      " - p8::MSP_n: GPR (CoP = 0.948)\n",
      " - p9::UWP_Mu: GPR (CoP = 0.912)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 9: Seleccionar el mejor modelo (mayor CoP) para cada variable de salida\n",
    "# =============================================================================\n",
    "best_models = {}\n",
    "for salida in cop_df.index:\n",
    "    best_model = cop_df.loc[salida].idxmax()\n",
    "    best_models[salida] = best_model\n",
    "\n",
    "print(\"Mejor modelo para cada variable de salida basado en CoP:\")\n",
    "for output in Y_test_scaled.columns:\n",
    "    bm = best_models.get(output)\n",
    "    if bm is None:\n",
    "        print(f\" - {output}: No se evaluó ningún modelo\")\n",
    "    else:\n",
    "        print(f\" - {output}: {bm} (CoP = {cop_results[output][bm]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b24e9632-a892-4ff7-a39f-fdf2a26cf9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del DataFrame final:\n",
      "    x1::OSD  x2::Dint     x3::L    x4::tm   x5::hs2    x6::wt    x7::Nt  \\\n",
      "0 -0.343667 -1.220977  0.649212  0.381250  1.272265 -0.189762  1.360000   \n",
      "1  0.449846 -1.157263  1.416511 -0.265316  0.948643 -1.547684 -0.900907   \n",
      "2  1.153337 -0.370169 -0.178523  0.805775  1.533912 -1.171137  0.041137   \n",
      "3  1.386379 -0.980225  0.253293 -0.235001  2.418004  0.694772  0.417955   \n",
      "4 -1.349679 -0.043280 -0.938649  1.348390 -1.151218 -0.976581 -1.089316   \n",
      "\n",
      "     x8::Nh     p1::W   p4::GFF  p5::BSP_T  p6::BSP_n  p7::BSP_Mu  p8::MSP_n  \\\n",
      "0 -0.496430  0.437130  0.398909   1.140166  -1.150828   -2.133137  -1.850668   \n",
      "1  0.586261  0.912531 -2.117615   0.021198  -0.008952    0.342648   0.681511   \n",
      "2  1.668951  0.370108 -0.144116   0.763857  -0.198446   -0.183838   0.561392   \n",
      "3  0.044915  0.658452 -0.932911   0.658856  -0.815164   -0.916397  -0.468676   \n",
      "4  1.668951 -1.099657  1.178947  -0.923586   3.061111    0.265404   0.780434   \n",
      "\n",
      "   p9::UWP_Mu  \n",
      "0    1.030106  \n",
      "1   -0.005790  \n",
      "2    0.507358  \n",
      "3    0.842809  \n",
      "4   -1.112518  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 10: Crear un DataFrame final combinando X escalado y las predicciones del mejor modelo\n",
    "# =============================================================================\n",
    "# Obtener las predicciones sobre el conjunto completo X_scaled_df para cada modelo\n",
    "predicciones_totales = {\n",
    "    'PLS': model_PLS.predict(X_scaled_df),\n",
    "    'LR': model_LR.predict(X_scaled_df),\n",
    "    'GPR': model_kriging.predict(X_scaled_df),\n",
    "    'SVR': model_svr.predict(X_scaled_df),\n",
    "    'RF': model_rf.predict(X_scaled_df)\n",
    "}\n",
    "\n",
    "# Combinar las predicciones: para cada salida se usa la predicción del modelo con mayor CoP\n",
    "pred_final = np.zeros_like(predicciones_totales['PLS'])\n",
    "for j, salida in enumerate(Y.columns):\n",
    "    modelo_mejor = best_models[salida]\n",
    "    pred_final[:, j] = predicciones_totales[modelo_mejor][:, j]\n",
    "\n",
    "# Crear el DataFrame final con las variables de entrada escaladas y las predicciones\n",
    "final_df = pd.concat([X_scaled_df, pd.DataFrame(pred_final, columns=Y.columns, index=X_scaled_df.index)], axis=1)\n",
    "print(\"Primeras filas del DataFrame final:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9440a60-e7c2-450f-a767-66e0d9d4259f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1::OSD</th>\n",
       "      <th>x2::Dint</th>\n",
       "      <th>x3::L</th>\n",
       "      <th>x4::tm</th>\n",
       "      <th>x5::hs2</th>\n",
       "      <th>x6::wt</th>\n",
       "      <th>x7::Nt</th>\n",
       "      <th>x8::Nh</th>\n",
       "      <th>p1::W</th>\n",
       "      <th>p4::GFF</th>\n",
       "      <th>p5::BSP_T</th>\n",
       "      <th>p6::BSP_n</th>\n",
       "      <th>p7::BSP_Mu</th>\n",
       "      <th>p8::MSP_n</th>\n",
       "      <th>p9::UWP_Mu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.690000</td>\n",
       "      <td>21.320000</td>\n",
       "      <td>25.140000</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>11.260000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.547394</td>\n",
       "      <td>51.711677</td>\n",
       "      <td>0.618867</td>\n",
       "      <td>3120.416189</td>\n",
       "      <td>81.462192</td>\n",
       "      <td>4959.426327</td>\n",
       "      <td>91.695487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.385006</td>\n",
       "      <td>21.566435</td>\n",
       "      <td>31.920320</td>\n",
       "      <td>2.741117</td>\n",
       "      <td>10.541636</td>\n",
       "      <td>2.004243</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.627553</td>\n",
       "      <td>24.311058</td>\n",
       "      <td>0.384309</td>\n",
       "      <td>10799.696304</td>\n",
       "      <td>89.618753</td>\n",
       "      <td>9876.090239</td>\n",
       "      <td>87.702947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58.660824</td>\n",
       "      <td>24.610770</td>\n",
       "      <td>17.825636</td>\n",
       "      <td>3.236242</td>\n",
       "      <td>11.840792</td>\n",
       "      <td>2.327503</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.536093</td>\n",
       "      <td>45.799069</td>\n",
       "      <td>0.539985</td>\n",
       "      <td>9525.319148</td>\n",
       "      <td>87.884226</td>\n",
       "      <td>9642.857284</td>\n",
       "      <td>89.680719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.745990</td>\n",
       "      <td>22.251184</td>\n",
       "      <td>21.641420</td>\n",
       "      <td>2.755130</td>\n",
       "      <td>13.803262</td>\n",
       "      <td>3.929360</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.584712</td>\n",
       "      <td>37.210453</td>\n",
       "      <td>0.517975</td>\n",
       "      <td>5377.803194</td>\n",
       "      <td>85.470786</td>\n",
       "      <td>7642.802329</td>\n",
       "      <td>90.973608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.005493</td>\n",
       "      <td>25.875113</td>\n",
       "      <td>11.108705</td>\n",
       "      <td>3.487073</td>\n",
       "      <td>5.880454</td>\n",
       "      <td>2.494527</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.288267</td>\n",
       "      <td>60.204944</td>\n",
       "      <td>0.186264</td>\n",
       "      <td>31446.308542</td>\n",
       "      <td>89.364268</td>\n",
       "      <td>10068.165355</td>\n",
       "      <td>83.437409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1::OSD   x2::Dint      x3::L    x4::tm    x5::hs2    x6::wt  x7::Nt  \\\n",
       "0  51.690000  21.320000  25.140000  3.040000  11.260000  3.170000    18.0   \n",
       "1  55.385006  21.566435  31.920320  2.741117  10.541636  2.004243     6.0   \n",
       "2  58.660824  24.610770  17.825636  3.236242  11.840792  2.327503    11.0   \n",
       "3  59.745990  22.251184  21.641420  2.755130  13.803262  3.929360    13.0   \n",
       "4  47.005493  25.875113  11.108705  3.487073   5.880454  2.494527     5.0   \n",
       "\n",
       "   x8::Nh     p1::W    p4::GFF  p5::BSP_T     p6::BSP_n  p7::BSP_Mu  \\\n",
       "0     4.0  0.547394  51.711677   0.618867   3120.416189   81.462192   \n",
       "1     6.0  0.627553  24.311058   0.384309  10799.696304   89.618753   \n",
       "2     8.0  0.536093  45.799069   0.539985   9525.319148   87.884226   \n",
       "3     5.0  0.584712  37.210453   0.517975   5377.803194   85.470786   \n",
       "4     8.0  0.288267  60.204944   0.186264  31446.308542   89.364268   \n",
       "\n",
       "      p8::MSP_n  p9::UWP_Mu  \n",
       "0   4959.426327   91.695487  \n",
       "1   9876.090239   87.702947  \n",
       "2   9642.857284   89.680719  \n",
       "3   7642.802329   90.973608  \n",
       "4  10068.165355   83.437409  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 229 entries, 0 to 228\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   x1::OSD     229 non-null    float64\n",
      " 1   x2::Dint    229 non-null    float64\n",
      " 2   x3::L       229 non-null    float64\n",
      " 3   x4::tm      229 non-null    float64\n",
      " 4   x5::hs2     229 non-null    float64\n",
      " 5   x6::wt      229 non-null    float64\n",
      " 6   x7::Nt      229 non-null    float64\n",
      " 7   x8::Nh      229 non-null    float64\n",
      " 8   p1::W       229 non-null    float64\n",
      " 9   p4::GFF     229 non-null    float64\n",
      " 10  p5::BSP_T   229 non-null    float64\n",
      " 11  p6::BSP_n   229 non-null    float64\n",
      " 12  p7::BSP_Mu  229 non-null    float64\n",
      " 13  p8::MSP_n   229 non-null    float64\n",
      " 14  p9::UWP_Mu  229 non-null    float64\n",
      "dtypes: float64(15)\n",
      "memory usage: 27.0 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame desescalado guardado en: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\400_MOT_Optimizado\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 10.1: Desescalar el DataFrame final y guardar en CSV\n",
    "# =============================================================================\n",
    "\n",
    "# Separar las columnas de entrada y de salida (recordando que 'features' y 'Y.columns' ya están definidos)\n",
    "X_final_scaled = final_df[features]\n",
    "Y_final_scaled = final_df[Y.columns]\n",
    "\n",
    "# Aplicar el inverso de la transformación para obtener los valores originales (desescalados)\n",
    "X_final_unscaled = pd.DataFrame(scaler_X.inverse_transform(X_final_scaled), \n",
    "                                columns=features, \n",
    "                                index=final_df.index)\n",
    "Y_final_unscaled = pd.DataFrame(scaler_Y.inverse_transform(Y_final_scaled), \n",
    "                                columns=Y.columns, \n",
    "                                index=final_df.index)\n",
    "\n",
    "# Combinar las variables desescaladas en un único DataFrame\n",
    "final_unscaled_df = pd.concat([X_final_unscaled, Y_final_unscaled], axis=1)\n",
    "display(final_unscaled_df.head())\n",
    "display(final_unscaled_df.info())\n",
    "\n",
    "# Definir la ruta de salida para guardar el CSV (puedes ajustar la carpeta y nombre del archivo)\n",
    "output_csv_path = os.path.join(figure_path, \"final_df_desescalado.csv\")\n",
    "\n",
    "# Guardar el DataFrame desescalado en formato CSV\n",
    "model_file = os.path.join(model_path, \"trained_database_optimal.csv\")\n",
    "final_unscaled_df.to_csv(model_file, index=True)\n",
    "print(\"DataFrame desescalado guardado en:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ef4d816-36bf-4841-bfcd-32fe7a5b2b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figura de comparación guardada en: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Figuras_MOP\\400_MOT_Optimizado\\valores_FEA_vs_valores_predichos.png\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 11: Graficar comparación (scatter) entre FEA (valores originales escalados)\n",
    "# y las predicciones, usando las métricas calculadas previamente para el mejor modelo\n",
    "# =============================================================================\n",
    "\n",
    "# Lista de variables de salida (asegúrate de que coincide con Y_scaled_df.columns)\n",
    "output_vars = Y_scaled_df.columns.tolist()\n",
    "n_vars = len(output_vars)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_vars / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "axes = axes.flatten()  # Asegurarse de tener una lista de ejes\n",
    "\n",
    "# Iterar sobre cada variable de salida\n",
    "for i, col in enumerate(output_vars):\n",
    "    # Extraer valores originales y predichos\n",
    "    y_true = Y_scaled_df[col].values\n",
    "    y_pred = final_df[col].values\n",
    "\n",
    "    # Obtener el mejor modelo para esta variable y extraer las métricas pre-calculadas\n",
    "    best_model = best_models[col]  # best_models ya fue calculado previamente (ej. {'p1::W': 'PLS', ...})\n",
    "    mse  = metricas[best_model]['MSE'][i]\n",
    "    r2   = metricas[best_model]['R2'][i]\n",
    "    cop  = metricas[best_model]['CoP'][i]\n",
    "    \n",
    "    # Dibujar el scatter en el subplot correspondiente (usar ax, no axes)\n",
    "    ax = axes[i]\n",
    "    ax.scatter(y_true, y_pred, alpha=0.6, edgecolor=\"k\")\n",
    "    \n",
    "    # Dibujar la línea de identidad\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    # Configurar etiquetas y título para el subplot\n",
    "    ax.set_title(f\"{col}\")\n",
    "    ax.set_xlabel(\"FEA Simulation (escalado)\")\n",
    "    ax.set_ylabel(\"Prediction (escalado)\")\n",
    "    \n",
    "    # Anotar el subplot con las métricas extraídas para el mejor modelo\n",
    "    annotation = f\"Mejor Modelo: {best_model}\\nCoP: {cop:.3f}\\nR²: {r2:.3f}\\nMSE: {mse:.3f}\"\n",
    "    ax.text(0.05, 0.95, annotation, transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5))\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Eliminar subplots vacíos si existen\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar la figura en la carpeta 'figure_path'\n",
    "figure_file = os.path.join(figure_path, \"valores_FEA_vs_valores_predichos.png\")\n",
    "plt.savefig(figure_file, dpi=1080)\n",
    "plt.close()\n",
    "print(\"Figura de comparación guardada en:\", figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37d61ee9-62d0-4f1c-a4cb-cd56ce99c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Paso 12: Calcular y representar mapas de calor de p₍ᵢⱼ₎ (correlación entre predicción y variables de entrada)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Diccionario con las predicciones de cada modelo en el conjunto de test\n",
    "modelos_pred = {\n",
    "    'PLS': predicciones_test_pls,\n",
    "    'LR': predicciones_test_lr,\n",
    "    'GPR': predicciones_test_kriging,\n",
    "    'SVR': predicciones_test_svr,\n",
    "    'RF': predicciones_test_rf\n",
    "}\n",
    "\n",
    "# Calcular p₍ᵢⱼ₎ para cada modelo\n",
    "p_values_by_model = {}\n",
    "for model_name, preds in modelos_pred.items():\n",
    "    preds = np.asarray(preds)  # Convertir a array de NumPy\n",
    "    # Crear un DataFrame para almacenar p₍ᵢⱼ₎ con índices = salidas, columnas = entradas\n",
    "    p_matrix = pd.DataFrame(index=Y_test_scaled.columns, columns=X_test_scaled.columns, dtype=float)\n",
    "    \n",
    "    # Para cada variable de salida (fila)\n",
    "    for i, out_var in enumerate(Y_test_scaled.columns):\n",
    "        y_pred = preds[:, i]\n",
    "        # Para cada variable de entrada (columna)\n",
    "        for in_var in X_test_scaled.columns:\n",
    "            x_vals = X_test_scaled[in_var].values\n",
    "            # Si x_vals tiene más de una dimensión, tomar la primera columna\n",
    "            if x_vals.ndim > 1:\n",
    "                x_vals = x_vals[:, 0]\n",
    "            # Calcular p₍ᵢⱼ₎ usando la función compute_corr\n",
    "            p_matrix.loc[out_var, in_var] = compute_corr(y_pred, x_vals)\n",
    "    \n",
    "    p_values_by_model[model_name] = p_matrix\n",
    "\n",
    "# Representar mapas de calor de p₍ᵢⱼ₎ para cada modelo y guardarlos\n",
    "for model_name, df_p in p_values_by_model.items():\n",
    "    pij_matrix = df_p.T.values  # Transponer para tener filas = entradas y columnas = salidas\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_heatmap(pij_matrix, \n",
    "                 col_labels=Y_test_scaled.columns.tolist(), \n",
    "                 row_labels=X_test_scaled.columns.tolist(),\n",
    "                 title=f\"Mapa de calor de p₍ᵢⱼ₎ para {model_name}\", ax=ax)\n",
    "    plt.tight_layout()\n",
    "    figure_file = os.path.join(figure_path, f\"Mapa_de_calor_pij_{model_name}.png\")\n",
    "    plt.savefig(figure_file, dpi=1080)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec0988d1-4056-4d03-aa95-068e061f2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Paso 13: Calcular y representar el mapa de calor de la correlación de Pearson (entradas vs salidas)\n",
    "# =============================================================================\n",
    "# Concatenar X escalado y Y escalado completos\n",
    "combined = pd.concat([X_scaled_df, Y_scaled_df], axis=1)\n",
    "corr_matrix = combined.corr()\n",
    "# Extraer la submatriz: filas = entradas, columnas = salidas\n",
    "pearson_matrix = corr_matrix.loc[X_scaled_df.columns, Y_scaled_df.columns]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_heatmap(pearson_matrix, col_labels=Y_scaled_df.columns.tolist(),\n",
    "             row_labels=X_scaled_df.columns.tolist(),\n",
    "             title=\"Matriz de correlación de Pearson (entradas vs salidas)\", ax=ax)\n",
    "plt.tight_layout()\n",
    "figure_file = os.path.join(figure_path, \"Mapa_de_calor_Pearson.png\")\n",
    "plt.savefig(figure_file, dpi=1080)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc12b621-2fe3-41cb-af03-a2ff973ff230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo ensemble ha sido guardado en: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\400_MOT_Optimizado\\best_model_ensemble.pkl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Paso 14: Almacenar el mejor modelo para cada variable de salida en un modelo único\n",
    "# =============================================================================\n",
    "import pickle\n",
    "\n",
    "# Se asume que:\n",
    "# - 'outputs' es la lista de nombres de las variables de salida.\n",
    "# - 'best_models' es un diccionario que mapea cada variable de salida a un nombre de modelo óptimo, por ejemplo:\n",
    "#       best_models = {'p1::W': 'PLS', 'p4::GFF': 'LR', 'p5::BSP_T': 'GPR', ...}\n",
    "# - Se tienen los modelos optimizados (reentrenados) para cada familia en variables:\n",
    "#       model_PLS_opt, model_LR_opt, model_GPR_opt, model_SVR_opt, model_RF_opt\n",
    "\n",
    "# Primero, creamos un diccionario que asocie el nombre de cada modelo con su instancia optimizada.\n",
    "optimized_models = {\n",
    "    'PLS': model_PLS,\n",
    "    'LR': model_LR,\n",
    "    'GPR': model_kriging,\n",
    "    'SVR': model_svr,\n",
    "    'RF': model_rf\n",
    "}\n",
    "\n",
    "# Ahora, para cada variable de salida, almacenamos en un diccionario una tupla (modelo, índice)\n",
    "# donde \"modelo\" es la instancia optimizada y \"índice\" es la posición de esa salida en el vector de salida.\n",
    "best_model_per_output = {}\n",
    "for i, output in enumerate(outputs):\n",
    "    # Selecciona el nombre del mejor modelo para esta salida (obtenido en pasos anteriores)\n",
    "    model_name = best_models[output]  # Ej.: 'GPR'\n",
    "    model_instance = optimized_models[model_name]\n",
    "    best_model_per_output[output] = (model_instance, i)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Definir una clase que encapsule el ensemble de los mejores modelos\n",
    "# -----------------------------------------------------------------------------\n",
    "class BestModelEnsemble:\n",
    "    def __init__(self, model_dict, outputs):\n",
    "        \"\"\"\n",
    "        model_dict: Diccionario que mapea cada variable de salida a una tupla (modelo, índice)\n",
    "                    donde 'modelo' es el mejor modelo para esa salida y 'índice' es la posición\n",
    "                    de esa salida en el vector de predicción que produce ese modelo.\n",
    "        outputs: Lista de nombres de variables de salida, en el orden deseado.\n",
    "        \"\"\"\n",
    "        self.model_dict = model_dict\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Realiza la predicción para cada variable de salida usando el modelo asignado.\n",
    "        Se espera que cada modelo tenga un método predict que devuelva un array de\n",
    "        dimensiones (n_samples, n_outputs_model). Si el modelo es univariable, se asume\n",
    "        que devuelve un array 1D.\n",
    "        \n",
    "        :param X: Datos de entrada (array o DataFrame) con la forma (n_samples, n_features).\n",
    "        :return: Array con la predicción para todas las variables de salida, forma (n_samples, n_outputs).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = len(self.outputs)\n",
    "        preds = np.zeros((n_samples, n_outputs))\n",
    "        \n",
    "        # Iterar sobre cada variable de salida\n",
    "        for output in self.outputs:\n",
    "            model, idx = self.model_dict[output]\n",
    "            model_pred = model.predict(X)\n",
    "            # Si el modelo es univariable, model_pred es 1D; de lo contrario, es 2D\n",
    "            if model_pred.ndim == 1:\n",
    "                preds[:, self.outputs.index(output)] = model_pred\n",
    "            else:\n",
    "                preds[:, self.outputs.index(output)] = model_pred[:, idx]\n",
    "        return preds\n",
    "\n",
    "# Crear la instancia del ensemble\n",
    "ensemble_model = BestModelEnsemble(best_model_per_output, outputs)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "#preds_ensemble = ensemble_model.predict(X_scaled_df)\n",
    "#print(\"Forma de las predicciones del ensemble:\", preds_ensemble.shape)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Guardar el ensemble en un archivo para uso posterior\n",
    "# -----------------------------------------------------------------------------\n",
    "ensemble_file = os.path.join(model_path, \"best_model_ensemble.pkl\")\n",
    "with open(ensemble_file, \"wb\") as f:\n",
    "    pickle.dump(ensemble_model, f)\n",
    "print(\"El modelo ensemble ha sido guardado en:\", ensemble_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52bf0ff1-6332-462e-91f2-2c51151d0659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ensemble cargado correctamente desde: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\400_MOT_Optimizado\\best_model_ensemble.pkl\n",
      "\n",
      "Tabla de métricas del modelo ensemble (aplicado a X_train_scaled):\n",
      "              CoP     R²    MSE\n",
      "p1::W       0.997  0.996  0.004\n",
      "p4::GFF     0.975  0.975  0.025\n",
      "p5::BSP_T   0.993  0.992  0.008\n",
      "p6::BSP_n   0.973  0.973  0.027\n",
      "p7::BSP_Mu  0.972  0.972  0.028\n",
      "p8::MSP_n   0.972  0.972  0.028\n",
      "p9::UWP_Mu  0.972  0.971  0.029\n",
      "Figura de comparación guardada en: C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Figuras_MOP\\400_MOT_Optimizado\\Comparacion_FEA_vs_Predicciones.png\n"
     ]
    }
   ],
   "source": [
    "# Definir la ruta del archivo del ensemble (se asume que figure_path ya está definido)\n",
    "ensemble_file = os.path.join(model_path, \"best_model_ensemble.pkl\")\n",
    "\n",
    "# Cargar el modelo ensemble utilizando pickle\n",
    "with open(ensemble_file, \"rb\") as f:\n",
    "    loaded_ensemble = pickle.load(f)\n",
    "print(\"Modelo ensemble cargado correctamente desde:\", ensemble_file)\n",
    "\n",
    "# Obtener las predicciones del ensemble para los datos escalados (por ejemplo, el conjunto completo)\n",
    "preds_ensemble = loaded_ensemble.predict(X_scaled_df)\n",
    "\n",
    "# Lista de variables de salida (usando el DataFrame escalado de Y)\n",
    "output_vars = Y_scaled_df.columns.tolist()\n",
    "\n",
    "# Configurar la cuadrícula de subplots: una figura por cada variable de salida\n",
    "n_vars = len(output_vars)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_vars / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "axes = axes.flatten()  # Asegurarse de tener una lista de ejes\n",
    "\n",
    "# Inicializar una lista para almacenar las métricas por cada variable de salida\n",
    "metrics_results = []\n",
    "\n",
    "# Iterar sobre cada variable de salida y generar el scatter plot\n",
    "for i, col in enumerate(output_vars):\n",
    "    # Valores reales (FEA simulados, escalados) y predichos para la variable 'col'\n",
    "    y_true = Y_scaled_df[col].values\n",
    "    y_pred = preds_ensemble[:, i]\n",
    "    \n",
    "    # Calcular las métricas usando los valores ya obtenidos:\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    cop = compute_CoP(y_true, y_pred)\n",
    "\n",
    "    # Almacenar los resultados para la variable de salida actual\n",
    "    metrics_results.append([cop, r2, mse])\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.scatter(y_true, y_pred, alpha=0.6, edgecolor=\"k\")\n",
    "    \n",
    "    # Dibujar la línea de identidad (ideal: y = x)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    # Configurar etiquetas y título\n",
    "    ax.set_title(f\"{col}\", fontsize=12)\n",
    "    ax.set_xlabel(\"FEA Simulation (escalado)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Prediction (escalado)\", fontsize=10)\n",
    "    \n",
    "    # Anotar el subplot con las métricas calculadas para el mejor modelo\n",
    "    annotation = f\"CoP: {cop:.3f}\\nR²: {r2:.3f}\\nMSE: {mse:.3f}\"\n",
    "    ax.text(0.05, 0.95, annotation, transform=ax.transAxes,\n",
    "            verticalalignment=\"top\", fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5))\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Eliminar subplots vacíos (si existen)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Crear un DataFrame con los resultados de las métricas\n",
    "metrics_df = pd.DataFrame(metrics_results, columns=[\"CoP\", \"R²\", \"MSE\"], index = output_vars)\n",
    "print(\"\\nTabla de métricas del modelo ensemble (aplicado a X_train_scaled):\")\n",
    "print(metrics_df.round(3))\n",
    "\n",
    "# Guardar la figura en la carpeta 'figure_path'\n",
    "figure_file = os.path.join(figure_path, \"Comparacion_FEA_vs_Predicciones.png\")\n",
    "plt.savefig(figure_file, dpi=1080)\n",
    "plt.close()\n",
    "print(\"Figura de comparación guardada en:\", figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f46dd30-ebf7-4ef3-a944-d263ae4415ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecución completada.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Fin del código\n",
    "# =============================================================================\n",
    "print(\"Ejecución completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11289d-9976-4c82-a070-36ee3b350bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
