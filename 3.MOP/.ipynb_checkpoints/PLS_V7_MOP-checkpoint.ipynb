{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe6a29b-9eb9-4744-aa32-544f60294565",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae3baa-03f5-4257-bd0d-b0aad83e2ea2",
   "metadata": {},
   "source": [
    "## 3. MOP\n",
    "\n",
    "### 3.1. Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff92533-1e76-4f8f-9cf7-d0d175ed192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. MOP - Modelo de Optimización y Prognosis\n",
    "# =============================================================================\n",
    "\n",
    "# Librerías necesarias\n",
    "import os\n",
    "import re  # Import the regular expression module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Preprocesamiento y modelado\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1922bfe-c125-49ed-9d94-97ac70c11edf",
   "metadata": {},
   "source": [
    "### 3.2. Cargar y convertir los datos preprocesados en matrices X, M y P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b369df5f-7f54-4a94-8e28-e7a51fc84316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\DB_MOP\\design_DB_preprocessed_200_Optimizado.csv\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Figuras_MOP\\200_MOT_Optimizado\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\200_MOT_Optimizado\n"
     ]
    }
   ],
   "source": [
    "# Definir las rutas base y de las carpetas\n",
    "base_path = os.getcwd()  # Se asume que el notebook se ejecuta desde la carpeta 'MOP'\n",
    "db_path = os.path.join(base_path, \"DB_MOP\")\n",
    "fig_path = os.path.join(base_path, \"Figuras_MOP\")\n",
    "model_path = os.path.join(base_path, \"Modelos_MOP\")\n",
    "\n",
    "# Ruta al archivo de la base de datos\n",
    "data_file = os.path.join(db_path, \"design_DB_preprocessed_200_Optimizado.csv\")\n",
    "print(data_file)\n",
    "\n",
    "# Ruta al archivo de las figuras\n",
    "figure_path = os.path.join(fig_path, \"200_MOT_Optimizado\")\n",
    "print(figure_path)\n",
    "\n",
    "# Ruta al archivo de los modelos\n",
    "modelo_path = os.path.join(model_path, \"200_MOT_Optimizado\")\n",
    "print(modelo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5990deb-90f7-4e71-9caa-7a16036bf407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Lectura del archivo CSV\n",
    "try:\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(\"Archivo cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado. Revisa la ruta del archivo.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Problema al analizar el archivo CSV. Revisa el formato del archivo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado: {e}\")\n",
    "\n",
    "# Función para limpiar nombres de archivo inválidos\n",
    "def clean_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa110f43-c898-44ba-8701-3acadbccc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa las columnas en matrices X, M y P\n",
    "X_cols = [col for col in df.columns if col.startswith('x')]\n",
    "M_cols = [col for col in df.columns if col.startswith('m')]\n",
    "P_cols = [col for col in df.columns if col.startswith('p')]\n",
    "\n",
    "X = df[X_cols].copy()\n",
    "M = df[M_cols].copy()\n",
    "P = df[P_cols].copy()\n",
    "\n",
    "# Transforma todos los datos de X, M y P a numéricos\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "for col in M.columns:\n",
    "    M[col] = pd.to_numeric(M[col], errors='coerce')\n",
    "\n",
    "for col in P.columns:\n",
    "    P[col] = pd.to_numeric(P[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632978ca-2b07-400a-9d96-806bcb147571",
   "metadata": {},
   "source": [
    "### 3.3. Entrenamiento para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1012dc02-4ab8-4f06-a9a8-2b5eac0a187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de entrada: ['x1::OSD', 'x2::Dint', 'x3::L', 'x4::tm', 'x5::hs2', 'x6::wt', 'x7::Nt', 'x8::Nh', 'm1::Drot', 'm2::Dsh', 'm3::he', 'm4::Rmag', 'm5::Rs', 'm6::GFF']\n",
      "Variables de salida: ['p1::W', 'p4::GFF', 'p5::BSP_T', 'p6::BSP_n', 'p7::BSP_Mu', 'p8::MSP_n', 'p9::UWP_Mu']\n"
     ]
    }
   ],
   "source": [
    "# Las variables de salida se toman de la matriz P. Se eliminan 'p2::Tnom' y 'p3::nnom'\n",
    "outputs = [col for col in P.columns]\n",
    "if 'p2::Tnom' in outputs: outputs.remove('p2::Tnom')\n",
    "if 'p3::nnom' in outputs: outputs.remove('p3::nnom')\n",
    "\n",
    "# Concatena las matrices X y M\n",
    "X_M = pd.concat([X, M], axis=1)\n",
    "\n",
    "# Las entradas serán el resto de las columnas (tanto X como M)\n",
    "features = [col for col in X_M.columns]\n",
    "\n",
    "print(\"Variables de entrada:\", features)\n",
    "print(\"Variables de salida:\", outputs)\n",
    "\n",
    "X = df[features]\n",
    "Y = df[outputs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c863f8dd-22be-42ac-9241-fa530feee6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de datos\n",
    "# Se utiliza StandardScaler para normalizar los datos (importante para algunos modelos)\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)  # Datos de entrada escalados\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y)  # Datos de salida escalados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a562a51b-4edf-4b9d-b9f2-f88c53aad297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Separamos los conjuntos de datos en entrenamiento y test.\n",
    "# =============================================================================\n",
    "# Separar conjuntos de entrenamiento y prueba (80% / 20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Para asegurar que los DataFrames tengan índices consecutivos, se reinician\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test  = X_test.reset_index(drop=True)\n",
    "Y_train = Y_train.reset_index(drop=True)\n",
    "Y_test  = Y_test.reset_index(drop=True)\n",
    "\n",
    "# También, para los datos escalados (se convierten a DataFrame para conservar nombres de columnas)\n",
    "X_train_scaled = pd.DataFrame(scaler_X.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled  = pd.DataFrame(scaler_X.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "Y_train_scaled = pd.DataFrame(scaler_Y.transform(Y_train), columns=Y_train.columns)\n",
    "Y_test_scaled  = pd.DataFrame(scaler_Y.transform(Y_test), columns=Y_test.columns)\n",
    "\n",
    "# Crear DataFrames para el conjunto completo escalado (usado en reentrenamiento final)\n",
    "X_scaled_df = pd.DataFrame(scaler_X.transform(X), columns=X.columns, index=X.index)\n",
    "Y_scaled_df = pd.DataFrame(scaler_Y.transform(Y), columns=Y.columns, index=Y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e880a15-9b1e-412f-8053-9d78bbb1314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Se entrenan los modelos por separado.\n",
    "#==============================================================================\n",
    "# --- PLS (componentes optimizados) ---\n",
    "model_PLS = PLSRegression(n_components = 9)\n",
    "model_PLS.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_pls = model_PLS.predict(X_test_scaled)\n",
    "\n",
    "# --- Regresión lineal (LR) ---\n",
    "model_LR = LinearRegression()\n",
    "model_LR.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_lr = model_LR.predict(X_test_scaled)\n",
    "\n",
    "# --- KRIGING (GPR) ---\n",
    "kernel = RBF(length_scale=3.74, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "               WhiteKernel(noise_level=0.00211, noise_level_bounds=(1e-8, 100))\n",
    "model_kriging = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)\n",
    "model_kriging.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_kriging = model_kriging.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo Support Vector Regression (SVR) --- \n",
    "model_svr = MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "model_svr.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_svr = model_svr.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo Random Forest (RF)---\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_rf = model_rf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6a5872-7ade-422d-b968-22376ccc6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 1. Calcular métricas para cada modelo y cada variable de salida en el conjunto de test\n",
    "# -----------------------------------------------------------------------------\n",
    "# Diccionario que asocia cada modelo con sus predicciones sobre el test set\n",
    "modelos = {\n",
    "    'PLS': (model_PLS, predicciones_test_pls),\n",
    "    'LR': (model_LR, predicciones_test_lr),\n",
    "    'GPR': (model_kriging, predicciones_test_kriging),\n",
    "    'SVR': (model_svr, predicciones_test_svr),\n",
    "    'RF': (model_rf, predicciones_test_rf)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e20ef8-a2a3-4c6d-a3dd-1d58cc7d8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un diccionario para almacenar las métricas\n",
    "metricas = {nombre: {} for nombre in modelos.keys()}\n",
    "\n",
    "# Para cada modelo, calcular las métricas por cada variable de salida\n",
    "for nombre, (modelo, preds) in modelos.items():\n",
    "    metricas[nombre]['MSE'] = []\n",
    "    metricas[nombre]['RMSE'] = []\n",
    "    metricas[nombre]['MAE'] = []\n",
    "    metricas[nombre]['R2'] = []\n",
    "    metricas[nombre]['CoP'] = []\n",
    "    # Se itera sobre cada variable (columna) de salida\n",
    "    for i, col in enumerate(Y_test_scaled.columns):\n",
    "        y_true = Y_test_scaled[col].values\n",
    "        y_pred = preds[:, i]\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        # Cálculo del CoP según:\n",
    "        # CoP = ( [∑(y_true - media_y_true) * (y_pred - media_y_pred)] / ((N-1)*std_y_true*std_y_pred) )^2\n",
    "        N = len(y_true)\n",
    "        media_y = np.mean(y_true)\n",
    "        media_y_pred = np.mean(y_pred)\n",
    "        std_y = np.std(y_true, ddof=1)\n",
    "        std_y_pred = np.std(y_pred, ddof=1)\n",
    "        denominador = (N - 1) * std_y * std_y_pred\n",
    "        cop = (np.sum((y_true - media_y) * (y_pred - media_y_pred)) / denominador) ** 2 if denominador != 0 else np.nan\n",
    "        \n",
    "        metricas[nombre]['MSE'].append(mse)\n",
    "        metricas[nombre]['RMSE'].append(rmse)\n",
    "        metricas[nombre]['MAE'].append(mae)\n",
    "        metricas[nombre]['R2'].append(r2)\n",
    "        metricas[nombre]['CoP'].append(cop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc75b970-48ba-43c6-9ad9-b0b44a8d48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de CoP (cada fila es una variable de salida y cada columna un modelo):\n",
      "                 PLS        LR       GPR       SVR        RF\n",
      "p1::W       0.978726  0.978322  0.976231  0.949012  0.836816\n",
      "p4::GFF     0.996847  0.996860  0.989599  0.955832  0.870374\n",
      "p5::BSP_T   0.882227  0.881742  0.973143  0.931743  0.811409\n",
      "p6::BSP_n   0.858882  0.866434  0.943312  0.862322  0.885187\n",
      "p7::BSP_Mu  0.874045  0.884179  0.915923  0.848212  0.820263\n",
      "p8::MSP_n   0.789972  0.791059  0.881436  0.826073  0.739720\n",
      "p9::UWP_Mu  0.718808  0.710744  0.835483  0.798224  0.757564\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. Representar los valores de CoP para cada modelo y variable de salida\n",
    "# -----------------------------------------------------------------------------\n",
    "# Crear un DataFrame donde las filas sean las variables de salida y las columnas los modelos\n",
    "cop_df = pd.DataFrame({nombre: metricas[nombre]['CoP'] for nombre in modelos.keys()},\n",
    "                      index=Y_test_scaled.columns)\n",
    "\n",
    "print(\"Valores de CoP (cada fila es una variable de salida y cada columna un modelo):\")\n",
    "print(cop_df)\n",
    "\n",
    "# Graficar los valores de CoP\n",
    "cop_df.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Coefficient of Prognosis (CoP) por modelo y variable de salida\")\n",
    "plt.ylabel(\"CoP\")\n",
    "plt.xlabel(\"Variable de salida\")\n",
    "plt.legend(title=\"Modelo\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e089673f-89ae-4655-a575-aec20b5c9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir cop_results a partir de 'metricas'\n",
    "cop_results = {}\n",
    "for j, output in enumerate(outputs):\n",
    "    cop_results[output] = {}\n",
    "    for model_name in metricas.keys():\n",
    "         cop_results[output][model_name] = metricas[model_name]['CoP'][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64e821dc-678b-4028-a5ff-fdbe70715f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar gráficamente los CoP de cada modelo para cada salida en subplots\n",
    "# =============================================================================\n",
    "n_out = len(outputs)\n",
    "ncols = 3\n",
    "nrows = ceil(n_out / ncols)\n",
    "fig1, axes1 = plt.subplots(nrows, ncols, figsize=(12, 6 * nrows))\n",
    "if n_out == 1:\n",
    "    axes1 = [axes1]\n",
    "else:\n",
    "    axes1 = axes1.flatten()\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    # Se obtienen los nombres de los modelos y sus correspondientes CoP para la salida 'output'\n",
    "    model_names = list(cop_results[output].keys())\n",
    "    cop_vals = [cop_results[output][m] for m in model_names]\n",
    "    \n",
    "    ax = axes1[i]\n",
    "    ax.bar(model_names, cop_vals, color=\"steelblue\")\n",
    "    ax.set_title(f\"CoP para {output}\", fontsize=14)\n",
    "    ax.set_ylabel(\"CoP\", fontsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Eliminar subplots vacíos, si existen\n",
    "for j in range(i + 1, len(axes1)):\n",
    "    fig1.delaxes(axes1[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar la figura en la carpeta 'figure_path'\n",
    "figure_file = os.path.join(figure_path, 'CoP_para_cada_modelo.png')\n",
    "plt.savefig(figure_file, dpi=1080)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "386b289e-5231-476b-ba32-a42f85d985ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo para cada variable de salida basado en CoP:\n",
      " - p1::W: PLS (CoP = 0.979)\n",
      " - p4::GFF: LR (CoP = 0.997)\n",
      " - p5::BSP_T: GPR (CoP = 0.973)\n",
      " - p6::BSP_n: GPR (CoP = 0.943)\n",
      " - p7::BSP_Mu: GPR (CoP = 0.916)\n",
      " - p8::MSP_n: GPR (CoP = 0.881)\n",
      " - p9::UWP_Mu: GPR (CoP = 0.835)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. Seleccionar el mejor modelo (mayor CoP) para cada variable de salida\n",
    "# -----------------------------------------------------------------------------\n",
    "best_models = {}\n",
    "for salida in cop_df.index:\n",
    "    best_model = cop_df.loc[salida].idxmax()\n",
    "    best_models[salida] = best_model\n",
    "\n",
    "print(\"Mejor modelo para cada variable de salida basado en CoP:\")\n",
    "\n",
    "for output in outputs:\n",
    "    best_model = best_models.get(output)\n",
    "    if best_model is None:\n",
    "        print(f\" - {output}: No se evaluó ningún modelo (posiblemente la variable es constante o hubo error)\")\n",
    "    else:\n",
    "        print(f\" - {output}: {best_model} (CoP = {cop_results[output][best_model]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24e9632-a892-4ff7-a39f-fdf2a26cf9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del DataFrame final (X escalado y Y predicha con el mejor modelo por variable):\n",
      "    x1::OSD  x2::Dint     x3::L    x4::tm   x5::hs2    x6::wt    x7::Nt  \\\n",
      "0 -1.745484 -1.091625  0.082287  0.860428  0.578235 -0.551567  1.367565   \n",
      "1 -0.576354 -1.018010  1.005932  0.048988  0.277302 -2.109603 -0.921267   \n",
      "2  0.460140 -0.108615 -0.914109  1.393208  0.821538 -1.677565  0.032413   \n",
      "3  0.803495 -0.813464 -0.394306  0.087034  1.643646  0.463319  0.413885   \n",
      "4 -3.227699  0.269066 -1.829119  2.074190 -1.675338 -1.454338 -1.112003   \n",
      "\n",
      "     x8::Nh  m1::Drot   m2::Dsh  ...  m4::Rmag    m5::Rs   m6::GFF     p1::W  \\\n",
      "0 -0.400158 -1.091625 -1.325245  ... -1.142016 -0.202284  1.136058 -0.442143   \n",
      "1  0.693131 -1.018010 -1.009146  ... -1.023443 -0.471686 -1.224087  0.294143   \n",
      "2  1.786421 -0.108615 -0.525363  ... -0.185759  0.805432  0.552491 -0.508628   \n",
      "3  0.146487 -0.813464 -0.820792  ... -0.820447  1.159723 -0.074768 -0.163880   \n",
      "4  1.786421  0.269066 -0.361372  ...  0.155371 -1.606450  1.049661 -2.477633   \n",
      "\n",
      "    p4::GFF  p5::BSP_T  p6::BSP_n  p7::BSP_Mu  p8::MSP_n  p9::UWP_Mu  \n",
      "0  1.056507   0.951060  -0.995970   -1.947175  -1.266792    0.755087  \n",
      "1 -1.209855  -0.437715   0.693816    0.511204   1.090123   -0.678023  \n",
      "2  0.582350   0.584378   0.117684    0.274661   1.090124    0.334616  \n",
      "3 -0.291542   0.363640  -0.604260   -0.615132  -0.008563    0.605347  \n",
      "4  1.330838  -1.467577   4.802683    0.540480   1.090124   -1.870282  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. Crear un DataFrame final con X escalado y las predicciones (Y escaladas)\n",
    "#    provenientes del mejor modelo para cada variable de salida.\n",
    "# -----------------------------------------------------------------------------\n",
    "# Primero, obtener las predicciones sobre todo el conjunto escalado de X\n",
    "predicciones_totales = {}\n",
    "predicciones_totales['PLS'] = model_PLS.predict(X_scaled_df)\n",
    "predicciones_totales['LR'] = model_LR.predict(X_scaled_df)\n",
    "predicciones_totales['GPR'] = model_kriging.predict(X_scaled_df)\n",
    "predicciones_totales['SVR'] = model_svr.predict(X_scaled_df)\n",
    "predicciones_totales['RF'] = model_rf.predict(X_scaled_df)\n",
    "\n",
    "# Combinar las predicciones: para cada variable de salida se elige la predicción del modelo con mayor CoP\n",
    "# Se asume que el orden de las columnas en Y_scaled_df es el mismo que en Y\n",
    "pred_final = np.zeros_like(predicciones_totales['PLS'])\n",
    "for j, salida in enumerate(Y.columns):\n",
    "    modelo_mejor = best_models[salida]\n",
    "    pred_final[:, j] = predicciones_totales[modelo_mejor][:, j]\n",
    "\n",
    "# Crear un DataFrame final que combine las variables de entrada escaladas y las predicciones\n",
    "final_df = pd.concat([X_scaled_df, pd.DataFrame(pred_final, columns=Y.columns, index=X_scaled_df.index)], axis=1)\n",
    "\n",
    "print(\"Primeras filas del DataFrame final (X escalado y Y predicha con el mejor modelo por variable):\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1934d33-417d-4785-b622-0e88d5334588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Supongamos que 'final_df' contiene las predicciones (columnas de salida)\n",
    "# y que 'Y_scaled_df' contiene los valores originales escalados para las salidas.\n",
    "# Se asume que ambas DataFrames tienen las mismas columnas correspondientes a las variables de salida.\n",
    "output_vars = Y_scaled_df.columns  # Lista de variables de salida\n",
    "\n",
    "# Definir el número de filas y columnas para los subplots\n",
    "n_vars = len(output_vars)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_vars / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Para cada variable de salida se calcula las métricas y se genera el subplot\n",
    "for i, col in enumerate(output_vars):\n",
    "    # Valores originales y predichos para la variable actual\n",
    "    y_true = Y_scaled_df[col].values\n",
    "    y_pred = final_df[col].values\n",
    "\n",
    "    # Calcular MSE y R2\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Calcular CoP según la fórmula:\n",
    "    # CoP = ( [∑(y_true - media_y_true) * (y_pred - media_y_pred)] / ((N-1)*std_y_true*std_y_pred) )^2\n",
    "    N = len(y_true)\n",
    "    mean_y = np.mean(y_true)\n",
    "    mean_y_pred = np.mean(y_pred)\n",
    "    std_y = np.std(y_true, ddof=1)\n",
    "    std_y_pred = np.std(y_pred, ddof=1)\n",
    "    denominador = (N - 1) * std_y * std_y_pred\n",
    "    cop = (np.sum((y_true - mean_y) * (y_pred - mean_y_pred)) / denominador) ** 2 if denominador != 0 else np.nan\n",
    "\n",
    "    # Generar el subplot para la variable 'col'\n",
    "    ax = axes[i]\n",
    "    ax.scatter(y_true, y_pred, alpha=0.6, edgecolor=\"k\")\n",
    "    \n",
    "    # Línea de identidad para referencia (y=x)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    ax.set_title(f\"{col}\")\n",
    "    ax.set_xlabel(\"FEA Simulation (escalado)\")\n",
    "    ax.set_ylabel(\"Prediction (escalado)\")\n",
    "    \n",
    "    # Anotar el subplot con las métricas\n",
    "    ax.text(0.05, 0.95, f\"CoP: {cop:.3f}\\nR²: {r2:.3f}\\nMSE: {mse:.3f}\", \n",
    "            transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5))\n",
    "    '''\n",
    "    ax.set_title(\n",
    "        f\"{output}\\nR²={r2:.3f}, CoP={cop:.3f}, MSE={mse:.3e}\",\n",
    "        fontsize=14\n",
    "    )\n",
    "    '''\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Eliminar subplots vacíos, si los hay\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "# Guardar la figura en la carpeta 'Figuras_MOP/(La carpeta que corresponda)'\n",
    "figure_file = os.path.join(figure_path, f\"valores_FEA_vs_valores_predichos.png\")\n",
    "plt.savefig(figure_file, dpi =1080)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3346a389-c55d-442c-be75-1f8202caeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la correlación (p_ij) usando la fórmula del paper:\n",
    "def compute_corr(y, x):\n",
    "    N = len(y)\n",
    "    mean_y = np.mean(y)\n",
    "    mean_x = np.mean(x)\n",
    "    std_y = np.std(y, ddof=1)\n",
    "    std_x = np.std(x, ddof=1)\n",
    "    return np.sum((y - mean_y) * (x - mean_x)) / ((N - 1) * std_y * std_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37d61ee9-62d0-4f1c-a4cb-cd56ce99c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que ya tenemos:\n",
    "# - X_test_scaled: DataFrame con variables de entrada escaladas.\n",
    "# - Y_test_scaled: DataFrame con variables de salida escaladas.\n",
    "# - predicciones_test_pls, predicciones_test_lr, predicciones_test_kriging, \n",
    "#   predicciones_test_svr, predicciones_test_rf: predicciones de cada modelo en el test.\n",
    "\n",
    "# Diccionario con las predicciones de cada modelo.\n",
    "modelos = {\n",
    "    'PLS': predicciones_test_pls,\n",
    "    'LR': predicciones_test_lr,\n",
    "    'GPR': predicciones_test_kriging,\n",
    "    'SVR': predicciones_test_svr,\n",
    "    'RF': predicciones_test_rf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f49fb1b-86be-4efb-9f65-dff90781645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 1. Calcular los valores de p_ij para cada modelo (correlación entre la predicción de cada salida y cada variable de entrada)\n",
    "# -----------------------------------------------------------------------------\n",
    "p_values_by_model = {}\n",
    "for model_name, preds in modelos.items():\n",
    "    # Utilizamos las predicciones del mejor modelo para la variable de salida 'out'\n",
    "    # Convertir preds a un array de NumPy en caso de que sea un tuple\n",
    "    preds = np.asarray(preds)\n",
    "    \n",
    "    # Crear un DataFrame con índices = variables de salida y columnas = variables de entrada\n",
    "    p_matrix = pd.DataFrame(index=Y_test_scaled.columns, columns=X_test_scaled.columns, dtype=float)\n",
    "    \n",
    "    # Para cada variable de salida (fila) y cada variable de entrada (columna)\n",
    "    for i, out_var in enumerate(Y_test_scaled.columns):\n",
    "        y_pred = preds[:, i]\n",
    "        for in_var in X_test_scaled.columns:\n",
    "            x_vals = X_test_scaled[in_var].values\n",
    "            p_matrix.loc[out_var, in_var] = compute_corr(y_pred, x_vals)\n",
    "    p_values_by_model[model_name] = p_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29adc575-e96b-4265-8e4e-1beaa0776aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(matrix, col_labels, row_labels, title, ax=None):\n",
    "    \"\"\"\n",
    "    Dibuja un mapa de calor de la matriz usando seaborn.\n",
    "    'col_labels' son las etiquetas de las columnas y 'row_labels' las de las filas.\n",
    "    Si se especifica 'ax', se dibuja en ese subplot; de lo contrario, crea uno nuevo.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    sns.heatmap(matrix, annot=True, fmt=\".2f\", xticklabels=col_labels,\n",
    "                yticklabels=row_labels, cmap=\"viridis\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e99816-0363-485f-b6c6-198b11707551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen las etiquetas de las variables de entrada y de salida:\n",
    "features = X_test_scaled.columns.tolist()  # Ej: ['x1::OSD', 'x2::Dint', ...]\n",
    "outputs = Y_test_scaled.columns.tolist()     # Ej: ['p1::W', 'p4::GFF', ...]\n",
    "\n",
    "# =============================================================================\n",
    "# Representar los p₍ᵢⱼ₎ en mapas de calor: filas = variables de entrada, columnas = salidas\n",
    "# Se genera un heatmap para cada modelo evaluado y se guarda en 'figure_path'\n",
    "# =============================================================================\n",
    "for model_name, df in p_values_by_model.items():\n",
    "    # Nuestro DataFrame 'df' tiene:\n",
    "    #    index: outputs (variables de salida)\n",
    "    #    columns: features (variables de entrada)\n",
    "    # Para que el heatmap tenga filas = features y columnas = outputs, se transpone:\n",
    "    pij_matrix = df.T.values  # dimensiones: (n_features, n_outputs)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_heatmap(pij_matrix, col_labels=outputs, row_labels=features,\n",
    "                 title=f\"Mapa de calor de p₍ᵢⱼ₎ para {model_name}\", ax=ax)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar la figura en la carpeta 'figure_path'\n",
    "    figure_file = os.path.join(figure_path, f\"Mapa_de_calor_pij_{model_name}.png\")\n",
    "    plt.savefig(figure_file, dpi=1080)\n",
    "    plt.close()\n",
    "    # Para ver en pantalla, se podría usar plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c29615-eee8-4bb7-bd62-9c007861f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar el mapa de calor de la matriz de correlación de Pearson (entradas vs salidas)\n",
    "# =============================================================================\n",
    "pearson_matrix = np.zeros((len(features), len(outputs)))\n",
    "for i, feat in enumerate(features):\n",
    "    for j, out in enumerate(outputs):\n",
    "        if df[feat].std() == 0 or df[out].std() == 0:\n",
    "            pearson_matrix[i, j] = np.nan\n",
    "        else:\n",
    "            pearson_matrix[i, j] = np.corrcoef(df[feat], df[out])[0, 1]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_heatmap(pearson_matrix, col_labels=outputs, row_labels=features,\n",
    "             title=\"Matriz de correlación de Pearson (entradas vs salidas)\", ax=ax)\n",
    "plt.tight_layout()\n",
    "# Guardar la figura en la carpeta 'Figuras_MOP/(La carpeta que corresponda)'\n",
    "figure_file = os.path.join(figure_path, 'Mapa de calor Pearson.png')\n",
    "plt.savefig(figure_file, dpi =1080)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f46dd30-ebf7-4ef3-a944-d263ae4415ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. Calcular coeficientes de Pearson y generar mapa de calor\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Concatenar las variables de entrada y las salidas originales escaladas\n",
    "combined = pd.concat([X_scaled_df, Y_scaled_df], axis=1)\n",
    "corr_matrix = combined.corr()\n",
    "\n",
    "# Extraer la submatriz de correlación entre las variables de salida y las de entrada\n",
    "pearson_matrix = corr_matrix.loc[X_scaled_df.columns, Y_scaled_df.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_heatmap(pearson_matrix, col_labels=outputs, row_labels=features,\n",
    "             title=\"Matriz de correlación de Pearson (entradas vs salidas)\", ax=ax)\n",
    "plt.tight_layout()\n",
    "# Guardar la figura en la carpeta 'Figuras_MOP/(La carpeta que corresponda)'\n",
    "figure_file = os.path.join(figure_path, f\"Mapa_de_calor_Pearson.png\")\n",
    "plt.savefig(figure_file, dpi =1080)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11289d-9976-4c82-a070-36ee3b350bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
