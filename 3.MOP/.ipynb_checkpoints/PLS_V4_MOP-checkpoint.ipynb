{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe6a29b-9eb9-4744-aa32-544f60294565",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae3baa-03f5-4257-bd0d-b0aad83e2ea2",
   "metadata": {},
   "source": [
    "## 3. MOP\n",
    "\n",
    "### 3.1. Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff92533-1e76-4f8f-9cf7-d0d175ed192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías necesarias\n",
    "import os\n",
    "import re  # Import the regular expression module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1922bfe-c125-49ed-9d94-97ac70c11edf",
   "metadata": {},
   "source": [
    "### 3.2. Cargar y convertir los datos preprocesados en matrices X, M y P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b369df5f-7f54-4a94-8e28-e7a51fc84316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\DB_MOP\\design_DB_preprocessed_200_Optimizado.csv\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Figuras_MOP\\200_MOT_Optimizado\n",
      "C:\\Users\\s00244\\Documents\\GitHub\\MotorDesignDataDriven\\Notebooks\\3.MOP\\Modelos_MOP\\200_MOT_Optimizado\n"
     ]
    }
   ],
   "source": [
    "# Definir las rutas base y de las carpetas\n",
    "base_path = os.getcwd()  # Se asume que el notebook se ejecuta desde la carpeta 'MOP'\n",
    "db_path = os.path.join(base_path, \"DB_MOP\")\n",
    "fig_path = os.path.join(base_path, \"Figuras_MOP\")\n",
    "model_path = os.path.join(base_path, \"Modelos_MOP\")\n",
    "\n",
    "# Ruta al archivo de la base de datos\n",
    "data_file = os.path.join(db_path, \"design_DB_preprocessed_200_Optimizado.csv\")\n",
    "print(data_file)\n",
    "\n",
    "# Ruta al archivo de las figuras\n",
    "figure_path = os.path.join(fig_path, \"200_MOT_Optimizado\")\n",
    "print(figure_path)\n",
    "\n",
    "# Ruta al archivo de los modelos\n",
    "modelo_path = os.path.join(model_path, \"200_MOT_Optimizado\")\n",
    "print(modelo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5990deb-90f7-4e71-9caa-7a16036bf407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Lectura del archivo CSV\n",
    "try:\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(\"Archivo cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado. Revisa la ruta del archivo.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Problema al analizar el archivo CSV. Revisa el formato del archivo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado: {e}\")\n",
    "\n",
    "# Función para limpiar nombres de archivo inválidos\n",
    "def clean_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa110f43-c898-44ba-8701-3acadbccc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa las columnas en matrices X, M y P\n",
    "X_cols = [col for col in df.columns if col.startswith('x')]\n",
    "M_cols = [col for col in df.columns if col.startswith('m')]\n",
    "P_cols = [col for col in df.columns if col.startswith('p')]\n",
    "\n",
    "X = df[X_cols].copy()\n",
    "M = df[M_cols].copy()\n",
    "P = df[P_cols].copy()\n",
    "\n",
    "# Transforma todos los datos de X, M y P a numéricos\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "for col in M.columns:\n",
    "    M[col] = pd.to_numeric(M[col], errors='coerce')\n",
    "\n",
    "for col in P.columns:\n",
    "    P[col] = pd.to_numeric(P[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632978ca-2b07-400a-9d96-806bcb147571",
   "metadata": {},
   "source": [
    "### 3.3. Entrenamiento para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1012dc02-4ab8-4f06-a9a8-2b5eac0a187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de entrada: ['x1::OSD', 'x2::Dint', 'x3::L', 'x4::tm', 'x5::hs2', 'x6::wt', 'x7::Nt', 'x8::Nh', 'm1::Drot', 'm2::Dsh', 'm3::he', 'm4::Rmag', 'm5::Rs', 'm6::GFF']\n",
      "Variables de salida: ['p1::W', 'p4::GFF', 'p5::BSP_T', 'p6::BSP_n', 'p7::BSP_Mu', 'p8::MSP_n', 'p9::UWP_Mu']\n"
     ]
    }
   ],
   "source": [
    "# Las variables de salida será las etiquetas de la matriz P\n",
    "outputs = [col for col in P.columns]\n",
    "outputs.remove('p2::Tnom')\n",
    "outputs.remove('p3::nnom')\n",
    "\n",
    "# Concatena las matrices X y M\n",
    "X_M = pd.concat([X, M], axis=1)\n",
    "\n",
    "# Las entradas serán el resto de las columnas (tanto X como M)\n",
    "features = [col for col in X_M.columns]\n",
    "\n",
    "print(\"Variables de entrada:\", features)\n",
    "print(\"Variables de salida:\", outputs)\n",
    "\n",
    "X = df[features]\n",
    "Y = df[outputs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c863f8dd-22be-42ac-9241-fa530feee6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Escalado de datos\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_M)\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(P)\n",
    "'''\n",
    "\n",
    "# Escalado de datos\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a562a51b-4edf-4b9d-b9f2-f88c53aad297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Separamos los conjuntos de datos en entrenamiento y test.\n",
    "# =============================================================================\n",
    "# Dividir la base de datos en entrenamiento y prueba (80% / 20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =============================================================================\n",
    "# Escalar datos de entrada para modelos sensibles a la escala (por ejemplo, GPR)\n",
    "# =============================================================================\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index =X_train.index)\n",
    "X_test_scaled = scaler_X.fit_transform(X_test)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index =X_test.index)\n",
    "#print(X_train_df.head())\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_train_scaled = scaler_Y.fit_transform(Y_train)\n",
    "Y_train_df = pd.DataFrame(Y_train_scaled, columns=Y_train.columns, index =Y_train.index)\n",
    "Y_test_scaled = scaler_Y.fit_transform(Y_test)\n",
    "Y_test_df = pd.DataFrame(Y_test_scaled, columns=Y_test.columns, index =Y_test.index)\n",
    "#print(Y_train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b392c4e-7a09-4968-a500-fa93866a75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Funciones de cálculo\n",
    "# =============================================================================\n",
    "def compute_CoP(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el Coefficient of Prognosis (CoP) según la fórmula:\n",
    "       CoP = (Pearson_corr(y_true, y_pred))^2\n",
    "    \"\"\"\n",
    "    if np.std(y_true) == 0 or np.std(y_pred) == 0:\n",
    "        return np.nan\n",
    "    r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    return r ** 2\n",
    "\n",
    "def compute_standardized_coefficients(model, X, y):\n",
    "    \"\"\"\n",
    "    Para modelos lineales (que tienen atributo coef_),\n",
    "    calcula los coeficientes estandarizados:\n",
    "        coef_std = coef * (std(X) / std(y))\n",
    "    \"\"\"\n",
    "    coef = model.coef_.ravel()\n",
    "    std_X = X.std().values\n",
    "    std_y = y.std()\n",
    "    return coef * std_X / std_y\n",
    "\n",
    "def compute_pij(model, X, dataset=\"train\"):\n",
    "    \"\"\"\n",
    "    Calcula p_ij para un modelo entrenado, es decir, para cada variable de entrada\n",
    "    se calcula la correlación de Pearson entre la predicción del modelo (Ŝ)\n",
    "    y esa variable, según la fórmula:\n",
    "    \n",
    "         p_ij = (1/(N-1)) * Σ_k ((ŷ(k) - μ_ŷ) (x_j(k) - μ_xj)) / (σ_ŷ σ_xj)\n",
    "         \n",
    "    Se evalúa en el conjunto X (por ejemplo, X_train).\n",
    "    Si alguna variable es constante (std=0), se asigna NaN para evitar errores.\n",
    "    \"\"\"\n",
    "    # Obtener predicciones sobre X (se asume que la salida es unidimensional)\n",
    "    y_pred = model.predict(X).ravel()\n",
    "    pij = []\n",
    "    for col in X.columns:\n",
    "        std_feature = X[col].std()\n",
    "        if std_feature == 0:\n",
    "            pij.append(np.nan)\n",
    "        else:\n",
    "            corr = np.corrcoef(y_pred, X[col].values)[0, 1]\n",
    "            pij.append(corr)\n",
    "    return np.array(pij)\n",
    "\n",
    "def compute_pij_2(model, y_pred, X):\n",
    "    \"\"\"\n",
    "    Calcula p_ij para un modelo entrenado, es decir, para cada variable de entrada\n",
    "    se calcula la correlación de Pearson entre la predicción del modelo (Ŝ)\n",
    "    y esa variable, según la fórmula:\n",
    "    \n",
    "         p_ij = (1/(N-1)) * Σ_k ((ŷ(k) - μ_ŷ) (x_j(k) - μ_xj)) / (σ_ŷ σ_xj)\n",
    "         \n",
    "    Se evalúa en el conjunto X (por ejemplo, X_train).\n",
    "    Si alguna variable es constante (std=0), se asigna NaN para evitar errores.\n",
    "    \"\"\"\n",
    "    # Obtener predicciones sobre X (se asume que la salida es unidimensional)\n",
    "    pij = []\n",
    "    for col in X.columns:\n",
    "        std_feature = X[col].std()\n",
    "        if std_feature == 0:\n",
    "            pij.append(np.nan)\n",
    "        else:\n",
    "            corr = np.corrcoef(y_pred, X[col].values)[0, 1]\n",
    "            pij.append(corr)\n",
    "    return np.array(pij)\n",
    "\n",
    "def compute_permutation_importance(model, X, y):\n",
    "    \"\"\"\n",
    "    Para modelos no lineales (o en general), se utiliza la importancia por\n",
    "    permutación para estimar el efecto de cada variable.\n",
    "    \"\"\"\n",
    "    result = permutation_importance(model, X, y, scoring=\"r2\", n_repeats=10, random_state=0)\n",
    "    return result.importances_mean\n",
    "\n",
    "def plot_heatmap(matrix, col_labels, row_labels, title, ax=None):\n",
    "    \"\"\"\n",
    "    Representa un mapa de calor a partir de la matriz dada usando seaborn.\n",
    "    Si se proporciona 'ax', se dibuja en ese subplot.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    sns.heatmap(matrix, annot=True, fmt=\".2f\", xticklabels=col_labels, \n",
    "                yticklabels=row_labels, cmap=\"viridis\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6425a49-3a6c-4903-9247-3825a61ae233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número óptimo de componentes para modelar PLS es: 9\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Definir modelos subrogados\n",
    "# Se utilizan: PLS, regresión lineal (LR) y modelo de Kriging (GPR)\n",
    "# =============================================================================\n",
    "\n",
    "#==============================================================================\n",
    "# PLS\n",
    "n_features = X_train_df.shape[1]\n",
    "# Determinar número óptimo de componentes usando validación cruzada\n",
    "mse = []\n",
    "componentes = np.arange(1, min(len(X.columns), 20))\n",
    "\n",
    "for n in componentes:\n",
    "    pls = PLSRegression(n_components=n)\n",
    "    scores = cross_val_score(pls, X_train_df, Y_train_df, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse.append(-scores.mean())\n",
    "# Selección óptima de componentes (menor MSE)\n",
    "n_componentes_optimos = componentes[np.argmin(mse)]\n",
    "print(f'El número óptimo de componentes para modelar PLS es: {n_componentes_optimos}')\n",
    "#==============================================================================\n",
    "\n",
    "#==============================================================================\n",
    "# Kriging\n",
    "# kernel = C(1.0, (1e-4, 1e4)) * RBF(1.0, (1e-3, 1e3)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-4, 1e2))\n",
    "# kernel = RBF(length_scale_bounds=(1e-1, 1e1)) + WhiteKernel(noise_level_bounds=(1e-5, 1e+4))\n",
    "# Define rangos para explorar\n",
    "param_grid = {\n",
    "    \"kernel\": [RBF(length_scale=ls, length_scale_bounds=(1e-2, 1e3)) + \n",
    "               WhiteKernel(noise_level=n, noise_level_bounds=(1e-8, 1e+1))\n",
    "               for ls in [0.1, 1.0, 10.0] for n in [1e-3, 1e-2, 1e-1]]\n",
    "}\n",
    "\n",
    "# Definir un kernel inicial con límites amplios para optimizar:\n",
    "# - RBF(length_scale) con límites en escala logarítmica\n",
    "# - WhiteKernel(noise_level) con límites amplios\n",
    "kernel = RBF(length_scale=3.74, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "               WhiteKernel(noise_level=0.00211, noise_level_bounds=(1e-8, 100))\n",
    "\n",
    "#==============================================================================\n",
    "# \"GPR\": lambda: GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, normalize_y=True),\n",
    "# \"GPR\": lambda: GaussianProcessRegressor(kernel=RBF() + WhiteKernel(), random_state=42),\n",
    "# \"SVR\": lambda: MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "models = {\n",
    "    \"PLS\": lambda: PLSRegression(n_components=n_componentes_optimos),\n",
    "    \"LR\": lambda: LinearRegression(),\n",
    "    \"GPR\": lambda: GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10),\n",
    "    \"SVR\": lambda: MultiOutputRegressor(SVR(kernel='rbf')),\n",
    "    \"RF\": lambda: RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d0ae7ad-7dc5-49c5-9af2-d4ed16c22d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados:\n",
      "OrderedDict({'kernel__k1__length_scale': 2.5199040898724525, 'kernel__k2__noise_level': 0.03878670030667085})\n",
      "Mejor score (neg MSE): -0.11384154738135603\n",
      "MSE en test: 6.113e-02\n",
      "R² en test: 0.939\n",
      "Kernel final optimizado:\n",
      "RBF(length_scale=3.74) + WhiteKernel(noise_level=0.00211)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "gpr = GaussianProcessRegressor(random_state=42, n_restarts_optimizer=10)\n",
    "grid_search = GridSearchCV(gpr, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, Y_train)  # Asegúrate de usar datos escalados\n",
    "print(\"Best kernel:\", grid_search.best_estimator_.kernel_)\n",
    "'''\n",
    "\n",
    "X = df[features]\n",
    "y = df[outputs]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled  = scaler.fit_transform(y)\n",
    "\n",
    "# Dividir los datos y escalar X (GPR es sensible a la escala)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instanciar el modelo GPR sin reinicios en el optimizador ya que BayesSearchCV se encargará de buscar\n",
    "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "         WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-8, 1e+2))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=0)\n",
    "\n",
    "'''\n",
    "# Usamos los parámetros obtenidos en un primer entrenamiento:\n",
    "kernel = RBF(length_scale=1.12, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "         WhiteKernel(noise_level=0.035, noise_level_bounds=(1e-8, 1e+7))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)\n",
    "'''\n",
    "\n",
    "# Definir el espacio de búsqueda para los hiperparámetros del kernel.\n",
    "# La notación \"kernel__k1__length_scale\" y \"kernel__k2__noise_level\" es la que usa scikit-learn\n",
    "# para acceder a los parámetros del kernel compuesto (k1 corresponde a RBF y k2 a WhiteKernel).\n",
    "param_space = {\n",
    "    \"kernel__k1__length_scale\": Real(1e-2, 1e3, prior=\"log-uniform\"),\n",
    "    \"kernel__k2__noise_level\": Real(1e-8, 1e+2, prior=\"log-uniform\")\n",
    "}\n",
    "# Configurar la optimización bayesiana con BayesSearchCV\n",
    "opt = BayesSearchCV(\n",
    "    estimator=gpr,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=50,           # número de iteraciones de búsqueda\n",
    "    cv=3,                # validación cruzada de 3 pliegues\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ejecutar la búsqueda sobre los datos escalados\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(opt.best_params_)\n",
    "print(\"Mejor score (neg MSE):\", opt.best_score_)\n",
    "\n",
    "# Evaluar el modelo optimizado en el conjunto de test\n",
    "y_pred = opt.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE en test: {mse:.3e}\")\n",
    "print(f\"R² en test: {r2:.3f}\")\n",
    "\n",
    "# Puedes también imprimir el kernel final ajustado:\n",
    "print(\"Kernel final optimizado:\")\n",
    "print(opt.best_estimator_.kernel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "928cebf1-2145-4ecd-a080-48ecd800bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionarios para almacenar resultados\n",
    "cop_results = {output: {} for output in outputs}      # CoP para cada modelo y salida\n",
    "pij_results = {output: {} for output in outputs}       # Parámetros p_ij para cada modelo y salida\n",
    "predictions = {output: {} for output in outputs}        # Predicciones en test para cada modelo y salida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05128a80-ae34-4354-beff-4b5e4fc7187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PLS (componentes optimizados) ---\n",
    "model_PLS = PLSRegression(n_components = n_componentes_optimos)\n",
    "model_PLS.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_pls = pls_final.predict(X_test_scaled)\n",
    "\n",
    "# --- Regresión lineal (LR) ---\n",
    "model_LR = LinearRegression()\n",
    "model_LR.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_lr = model_LR.predict(X_test_scaled)\n",
    "\n",
    "# --- KRIGING (GPR) ---\n",
    "kernel = RBF(length_scale=3.74, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "               WhiteKernel(noise_level=0.00211, noise_level_bounds=(1e-8, 100))\n",
    "model_kriging = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)\n",
    "model_kriging.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_kriging = model_kriging.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo Support Vector Regression (SVR) --- \n",
    "model_svr = MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "model_svr.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_svr = model_svr.predict(X_test_scaled)\n",
    "\n",
    "# --- Modelo Random Forest (RF)---\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train_scaled, Y_train_scaled)\n",
    "predicciones_test_rf = model_rf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7dae138-fe68-4c12-a581-45c79acbf5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing p_ij for PLS on p1::W: 'p1::W'\n",
      "Error computing p_ij for LR on p1::W: 'p1::W'\n",
      "Error computing p_ij for GPR on p1::W: 'p1::W'\n",
      "Error computing p_ij for SVR on p1::W: 'p1::W'\n",
      "Error computing p_ij for RF on p1::W: 'p1::W'\n",
      "Error computing p_ij for PLS on p4::GFF: 'p4::GFF'\n",
      "Error computing p_ij for LR on p4::GFF: 'p4::GFF'\n",
      "Error computing p_ij for GPR on p4::GFF: 'p4::GFF'\n",
      "Error computing p_ij for SVR on p4::GFF: 'p4::GFF'\n",
      "Error computing p_ij for RF on p4::GFF: 'p4::GFF'\n",
      "Error computing p_ij for PLS on p5::BSP_T: 'p5::BSP_T'\n",
      "Error computing p_ij for LR on p5::BSP_T: 'p5::BSP_T'\n",
      "Error computing p_ij for GPR on p5::BSP_T: 'p5::BSP_T'\n",
      "Error computing p_ij for SVR on p5::BSP_T: 'p5::BSP_T'\n",
      "Error computing p_ij for RF on p5::BSP_T: 'p5::BSP_T'\n",
      "Error computing p_ij for PLS on p6::BSP_n: 'p6::BSP_n'\n",
      "Error computing p_ij for LR on p6::BSP_n: 'p6::BSP_n'\n",
      "Error computing p_ij for GPR on p6::BSP_n: 'p6::BSP_n'\n",
      "Error computing p_ij for SVR on p6::BSP_n: 'p6::BSP_n'\n",
      "Error computing p_ij for RF on p6::BSP_n: 'p6::BSP_n'\n",
      "Error computing p_ij for PLS on p7::BSP_Mu: 'p7::BSP_Mu'\n",
      "Error computing p_ij for LR on p7::BSP_Mu: 'p7::BSP_Mu'\n",
      "Error computing p_ij for GPR on p7::BSP_Mu: 'p7::BSP_Mu'\n",
      "Error computing p_ij for SVR on p7::BSP_Mu: 'p7::BSP_Mu'\n",
      "Error computing p_ij for RF on p7::BSP_Mu: 'p7::BSP_Mu'\n",
      "Error computing p_ij for PLS on p8::MSP_n: 'p8::MSP_n'\n",
      "Error computing p_ij for LR on p8::MSP_n: 'p8::MSP_n'\n",
      "Error computing p_ij for GPR on p8::MSP_n: 'p8::MSP_n'\n",
      "Error computing p_ij for SVR on p8::MSP_n: 'p8::MSP_n'\n",
      "Error computing p_ij for RF on p8::MSP_n: 'p8::MSP_n'\n",
      "Error computing p_ij for PLS on p9::UWP_Mu: 'p9::UWP_Mu'\n",
      "Error computing p_ij for LR on p9::UWP_Mu: 'p9::UWP_Mu'\n",
      "Error computing p_ij for GPR on p9::UWP_Mu: 'p9::UWP_Mu'\n",
      "Error computing p_ij for SVR on p9::UWP_Mu: 'p9::UWP_Mu'\n",
      "Error computing p_ij for RF on p9::UWP_Mu: 'p9::UWP_Mu'\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    # Se omite si la salida es constante (ya filtrada previamente)\n",
    "    if Y_train_df[output].std() == 0:\n",
    "        print(f\"Skipping output {output} because it is constant.\")\n",
    "        continue\n",
    "\n",
    "    for model_name, model_constructor in models.items():\n",
    "        model = model_constructor()\n",
    "\n",
    "        try:\n",
    "            model.fit(X_train_scaled, Y_train_scaled)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name} for {output}: {e}\")\n",
    "            cop_results[output][model_name] = np.nan\n",
    "            pij_results[output][model_name] = np.full(n_features, np.nan)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            y_pred_test = model.predict(X_test_scaled)\n",
    "            cop = compute_CoP(Y_test_scaled, y_pred_test)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing CoP for {model_name} on {output}: {e}\")\n",
    "            cop = np.nan\n",
    "        cop_results[output][model_name] = cop\n",
    "        predictions[output][model_name] = y_pred_test\n",
    "\n",
    "        try:\n",
    "            pij = compute_pij_2(model, y_pred_test, X_train_df[output])\n",
    "            # pij = compute_pij(model, X_test_df[output][model_name])\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing p_ij for {model_name} on {output}: {e}\")\n",
    "            pij = np.full(n_features, np.nan)\n",
    "        pij_results[output][model_name] = pij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b92bb8d0-1abe-426d-99aa-55941eef5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error training SVR for p1::W: y must have at least two dimensions for multi-output regression but has only one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:660: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL: .\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error training SVR for p4::GFF: y must have at least two dimensions for multi-output regression but has only one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error training SVR for p5::BSP_T: y must have at least two dimensions for multi-output regression but has only one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error training SVR for p6::BSP_n: y must have at least two dimensions for multi-output regression but has only one.\n",
      "Error training SVR for p7::BSP_Mu: y must have at least two dimensions for multi-output regression but has only one.\n",
      "Error training SVR for p8::MSP_n: y must have at least two dimensions for multi-output regression but has only one.\n",
      "Error training SVR for p9::UWP_Mu: y must have at least two dimensions for multi-output regression but has only one.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Definir modelos subrogados\n",
    "# Se usan: PLS, regresión lineal (LR), GaussianProcessRegressor (GPR) y SVR.\n",
    "# Para GPR y SVR se utilizarán datos escalados.\n",
    "# =============================================================================\n",
    "# Entrenar cada modelo para cada variable de salida\n",
    "for output in outputs:\n",
    "    # Se omite si la salida es constante (ya filtrada previamente)\n",
    "    if Y_train_df[output].std() == 0:\n",
    "        print(f\"Skipping output {output} because it is constant.\")\n",
    "        continue\n",
    "    \n",
    "    y_train = Y_train_df[output]\n",
    "    y_test = Y_test_df[output]\n",
    "    \n",
    "    for model_name, model_constructor in models.items():\n",
    "        model = model_constructor()\n",
    "\n",
    "        try:\n",
    "            # Para SVR usar datos escalados; para PLS y LR usar datos originales.\n",
    "            # Para SVR, convertir y_train a 2D para MultiOutputRegressor.\n",
    "            if model_name in [\"LR\", \"SVR\"]:\n",
    "                if model_name == \"SVR\":\n",
    "                    #print(X_train_df)\n",
    "                    #print(y_train)\n",
    "                    model.fit(X_train_df, y_train.values.reshape(-1, 1))\n",
    "                    # model.fit(X_train_df, y_train.values.reshape(-1, 1))\n",
    "                    # model.fit(X_train, y_train.values.reshape(-1, 1))\n",
    "                if model_name == \"LR\":\n",
    "                    model.fit(X_train_scaled, y_train.values.reshape(-1, 1))\n",
    "                else:\n",
    "                    #model.fit(X_train, y_train)\n",
    "                    model.fit(X_train_df, y_train)\n",
    "            else:\n",
    "                model.fit(X_train_df, y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name} for {output}: {e}\")\n",
    "            cop_results[output][model_name] = np.nan\n",
    "            pij_results[output][model_name] = np.full(n_features, np.nan)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if model_name in [\"SVR\"]:\n",
    "                # y_pred_test = model.predict(X_test).ravel()\n",
    "                y_pred_test = model.predict(X_test_scaled).ravel()\n",
    "            if model_name in [\"LR\"]:\n",
    "                # y_pred_test = model.predict(X_test).ravel()\n",
    "                y_pred_test = model.predict(X_test_scaled).ravel()\n",
    "            else:\n",
    "                y_pred_test = model.predict(X_test_df)\n",
    "                # y_pred_test = model.predict(X_test_df).ravel()\n",
    "            cop = compute_CoP(y_test, y_pred_test)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing CoP for {model_name} on {output}: {e}\")\n",
    "            cop = np.nan\n",
    "        cop_results[output][model_name] = cop\n",
    "        predictions[output][model_name] = y_pred_test\n",
    "    \n",
    "        try:\n",
    "            if model_name in [\"SVR\"]:\n",
    "                pij = compute_pij(model, X_train_df)\n",
    "                # pij = compute_pij(model, X_train_scaled)\n",
    "            if model_name in [\"LR\"]:\n",
    "                pij = compute_pij(model, X_train_df)\n",
    "                # pij = compute_pij(model, X_train_scaled)\n",
    "            else:\n",
    "                pij = compute_pij(model, X_train_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing p_ij for {model_name} on {output}: {e}\")\n",
    "            pij = np.full(n_features, np.nan)\n",
    "        pij_results[output][model_name] = pij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65f52c56-4819-4f59-af26-d4dfa4743c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejores modelos por variable de salida:\n",
      "  p1::W: PLS (CoP = 0.098)\n",
      "  p4::GFF: PLS (CoP = 0.098)\n",
      "  p5::BSP_T: PLS (CoP = 0.098)\n",
      "  p6::BSP_n: PLS (CoP = 0.098)\n",
      "  p7::BSP_Mu: PLS (CoP = 0.098)\n",
      "  p8::MSP_n: PLS (CoP = 0.098)\n",
      "  p9::UWP_Mu: PLS (CoP = 0.098)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Comparar CoP y elegir el mejor modelo para cada variable de salida\n",
    "# =============================================================================\n",
    "best_models = {}\n",
    "for output in outputs:\n",
    "    # Se selecciona el modelo con mayor CoP\n",
    "    cop_dict = cop_results[output]\n",
    "    if len(cop_dict) == 0:\n",
    "        continue\n",
    "    best_model = max(cop_dict, key=lambda m: cop_dict[m] if not np.isnan(cop_dict[m]) else -np.inf)\n",
    "    best_models[output] = best_model\n",
    "    \n",
    "print(\"\\nMejores modelos por variable de salida:\")\n",
    "for output in outputs:\n",
    "    best_model = best_models.get(output)\n",
    "    if best_model is None:\n",
    "        print(f\"  {output}: No se evaluó ningún modelo (posiblemente la variable es constante o hubo error)\")\n",
    "    else:\n",
    "        print(f\"  {output}: {best_model} (CoP = {cop_results[output][best_model]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bebf768f-c37d-45b2-a905-e6da98d701fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar gráficamente los CoP de cada modelo para cada salida en subplots\n",
    "# =============================================================================\n",
    "n_out = len(outputs)\n",
    "ncols = 3\n",
    "nrows = ceil(n_out / ncols)\n",
    "fig1, axes1 = plt.subplots(nrows, ncols, figsize=(12, 6*nrows))\n",
    "if n_out == 1:\n",
    "    axes1 = [axes1]\n",
    "else:\n",
    "    axes1 = axes1.flatten()\n",
    "for i, output in enumerate(outputs):\n",
    "    model_names = list(cop_results[output].keys())\n",
    "    cop_vals = [cop_results[output][m] for m in model_names]\n",
    "    ax = axes1[i]\n",
    "    ax.bar(model_names, cop_vals, color=\"steelblue\")\n",
    "    ax.set_title(f\"CoP for {output}\", fontsize=14)\n",
    "    ax.set_ylabel(\"CoP\", fontsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02833ddb-438d-4491-a81f-cb8b589cab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\seaborn\\matrix.py:202: RuntimeWarning: All-NaN slice encountered\n",
      "  vmin = np.nanmin(calc_data)\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\seaborn\\matrix.py:207: RuntimeWarning: All-NaN slice encountered\n",
      "  vmax = np.nanmax(calc_data)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Representar los p_ij en mapas de calor: filas = features, columnas = salidas\n",
    "# Se genera un heatmap para cada modelo evaluado\n",
    "# =============================================================================\n",
    "for model_name in models.keys():\n",
    "    # Construir una matriz de p_ij: filas = features, columnas = outputs\n",
    "    pij_matrix = []\n",
    "    valid_outputs = []\n",
    "    for output in outputs:\n",
    "        if model_name in pij_results[output]:\n",
    "            pij_matrix.append(pij_results[output][model_name])\n",
    "            valid_outputs.append(output)\n",
    "    if len(pij_matrix) == 0:\n",
    "        continue\n",
    "    pij_matrix = np.array(pij_matrix).T  # dimensiones: (n_features, n_outputs)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_heatmap(pij_matrix, col_labels=valid_outputs, row_labels=features,\n",
    "                 title=f\"Mapa de calor de p_ij para {model_name}\", ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6b8635-3588-4bd7-b72b-464b0928de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Representar el mapa de calor de la matriz de correlación de Pearson (entradas vs salidas)\n",
    "# =============================================================================\n",
    "pearson_matrix = np.zeros((len(features), len(outputs)))\n",
    "for i, feat in enumerate(features):\n",
    "    for j, out in enumerate(outputs):\n",
    "        if df[feat].std() == 0 or df[out].std() == 0:\n",
    "            pearson_matrix[i, j] = np.nan\n",
    "        else:\n",
    "            pearson_matrix[i, j] = np.corrcoef(df[feat], df[out])[0, 1]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_heatmap(pearson_matrix, col_labels=outputs, row_labels=features,\n",
    "             title=\"Matriz de correlación de Pearson (entradas vs salidas)\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4278bbc4-637d-4e53-9b62-e2180c7a8d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\s00244\\AppData\\Local\\miniconda3\\envs\\UAX\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Con el mejor modelo para cada salida, reentrenar con todo el dataset y generar predicciones óptimas\n",
    "# =============================================================================\n",
    "optimal_predictions = {}\n",
    "for output in outputs:\n",
    "    if output not in best_models:\n",
    "        continue\n",
    "    model_name = best_models[output]\n",
    "    model = models[model_name]()\n",
    "    y_full = df[output]\n",
    "    try:\n",
    "        if model_name in [\"SVR\"]:\n",
    "            model.fit(X_scaled, y_full)\n",
    "            y_pred_full = model.predict(X_scaled).ravel()\n",
    "        else:\n",
    "            model.fit(X, y_full)\n",
    "            y_pred_full = model.predict(X).ravel()\n",
    "    except Exception as e:\n",
    "        print(f\"Error re-training {model_name} for {output}: {e}\")\n",
    "        y_pred_full = np.full(len(df), np.nan)\n",
    "    optimal_predictions[output] = y_pred_full\n",
    "\n",
    "# Crear un nuevo DataFrame con las predicciones óptimas\n",
    "df_optimal = X.copy()\n",
    "for output in outputs:\n",
    "    if output in optimal_predictions:\n",
    "        df_optimal[f\"{output}_pred\"] = optimal_predictions[output]\n",
    "    else:\n",
    "        df_optimal[f\"{output}_pred\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b617bcc-91ae-4b21-b1aa-c2bee95ae9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Subplots para comparar, para cada salida, los valores FEA vs. las predicciones óptimas.\n",
    "# Se filtran filas con NaN antes de calcular las métricas.\n",
    "# =============================================================================\n",
    "fig2, axes2 = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 6*nrows))\n",
    "if n_out == 1:\n",
    "    axes2 = [axes2]\n",
    "else:\n",
    "    axes2 = axes2.flatten()\n",
    "for i, output in enumerate(outputs):\n",
    "    y_true = df[output].values\n",
    "    y_pred = df_optimal[f\"{output}_pred\"].values\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"No valid data for metrics evaluation of {output}.\")\n",
    "        continue\n",
    "    y_true_valid = y_true[mask]\n",
    "    y_pred_valid = y_pred[mask]\n",
    "    r2 = r2_score(y_true_valid, y_pred_valid)\n",
    "    mse = mean_squared_error(y_true_valid, y_pred_valid)\n",
    "    cop_full = compute_CoP(y_true_valid, y_pred_valid)\n",
    "    ax = axes2[i]\n",
    "    ax.scatter(y_true_valid, y_pred_valid, alpha=0.6, edgecolor=\"k\")\n",
    "    ax.plot([y_true_valid.min(), y_true_valid.max()],\n",
    "            [y_true_valid.min(), y_true_valid.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel(\"FEA Simulation\", fontsize=12)\n",
    "    ax.set_ylabel(\"Surrogate Prediction\", fontsize=12)\n",
    "    ax.set_title(f\"{output}\\nR²={r2:.3f}, CoP={cop_full:.3f}, MSE={mse:.3e}\", fontsize=14)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a651b52f-57eb-469e-8da9-a04fe06537f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nuevo dataset generado: 'trained_database_optimal.csv'\n"
     ]
    }
   ],
   "source": [
    "# Guardar el nuevo dataset\n",
    "df_optimal.to_csv(\"trained_database_optimal.csv\", index=False)\n",
    "print(\"\\nNuevo dataset generado: 'trained_database_optimal.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
